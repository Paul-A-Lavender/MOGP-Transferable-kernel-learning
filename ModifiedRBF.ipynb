{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from gpytorch.kernels import Kernel\n",
    "import torch\n",
    "\n",
    "def postprocess_rbf(dist_mat):\n",
    "    return dist_mat.div_(-2).exp_()\n",
    "\n",
    "\n",
    "class RBFKernel(Kernel):\n",
    "    r\"\"\"\n",
    "    Computes a covariance matrix based on the RBF (squared exponential) kernel\n",
    "    between inputs :math:`\\mathbf{x_1}` and :math:`\\mathbf{x_2}`:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "       \\begin{equation*}\n",
    "          k_{\\text{RBF}}(\\mathbf{x_1}, \\mathbf{x_2}) = \\exp \\left( -\\frac{1}{2}\n",
    "          (\\mathbf{x_1} - \\mathbf{x_2})^\\top \\Theta^{-2} (\\mathbf{x_1} - \\mathbf{x_2}) \\right)\n",
    "       \\end{equation*}\n",
    "\n",
    "    where :math:`\\Theta` is a lengthscale parameter.\n",
    "    See :class:`gpytorch.kernels.Kernel` for descriptions of the lengthscale options.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        This kernel does not have an `outputscale` parameter. To add a scaling parameter,\n",
    "        decorate this kernel with a :class:`gpytorch.kernels.ScaleKernel`.\n",
    "\n",
    "    :param ard_num_dims: Set this if you want a separate lengthscale for each input\n",
    "        dimension. It should be `d` if x1 is a `n x d` matrix. (Default: `None`.)\n",
    "    :param batch_shape: Set this if you want a separate lengthscale for each batch of input\n",
    "        data. It should be :math:`B_1 \\times \\ldots \\times B_k` if :math:`\\mathbf x1` is\n",
    "        a :math:`B_1 \\times \\ldots \\times B_k \\times N \\times D` tensor.\n",
    "    :param active_dims: Set this if you want to compute the covariance of only\n",
    "        a few input dimensions. The ints corresponds to the indices of the\n",
    "        dimensions. (Default: `None`.)\n",
    "    :param lengthscale_prior: Set this if you want to apply a prior to the\n",
    "        lengthscale parameter. (Default: `None`)\n",
    "    :param lengthscale_constraint: Set this if you want to apply a constraint\n",
    "        to the lengthscale parameter. (Default: `Positive`.)\n",
    "    :param eps: The minimum value that the lengthscale can take (prevents\n",
    "        divide by zero errors). (Default: `1e-6`.)\n",
    "\n",
    "    :ivar torch.Tensor lengthscale: The lengthscale parameter. Size/shape of parameter depends on the\n",
    "        ard_num_dims and batch_shape arguments.\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.randn(10, 5)\n",
    "        >>> # Non-batch: Simple option\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        >>> # Non-batch: ARD (different lengthscale for each input dimension)\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=5))\n",
    "        >>> covar = covar_module(x)  # Output: LinearOperator of size (10 x 10)\n",
    "        >>>\n",
    "        >>> batch_x = torch.randn(2, 10, 5)\n",
    "        >>> # Batch: Simple option\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        >>> # Batch: different lengthscale for each batch\n",
    "        >>> covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(batch_shape=torch.Size([2])))\n",
    "        >>> covar = covar_module(x)  # Output: LinearOperator of size (2 x 10 x 10)\n",
    "    \"\"\"\n",
    "\n",
    "    has_lengthscale = True\n",
    "\n",
    "    def forward(self, x1, x2, diag=False, **params):\n",
    "        if (\n",
    "            x1.requires_grad\n",
    "            or x2.requires_grad\n",
    "            or (self.ard_num_dims is not None and self.ard_num_dims > 1)\n",
    "            or diag\n",
    "            or params.get(\"last_dim_is_batch\", False)\n",
    "        ):\n",
    "            x1_ = x1.div(self.lengthscale)\n",
    "            x2_ = x2.div(self.lengthscale)\n",
    "            return postprocess_rbf(self.covar_dist(x1_, x2_, square_dist=True, diag=diag, **params))\n",
    "        return RBFCovariance.apply(\n",
    "            x1,\n",
    "            x2,\n",
    "            self.lengthscale,\n",
    "            lambda x1, x2: self.covar_dist(x1, x2, square_dist=True, diag=False, **params),\n",
    "        )\n",
    "    \n",
    "    \n",
    "class RBFCovariance(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x1, x2, lengthscale, sq_dist_func):\n",
    "        if any(ctx.needs_input_grad[:2]):\n",
    "            raise RuntimeError(\"RBFCovariance cannot compute gradients with \" \"respect to x1 and x2\")\n",
    "        if lengthscale.size(-1) > 1:\n",
    "            raise ValueError(\"RBFCovariance cannot handle multiple lengthscales\")\n",
    "        needs_grad = any(ctx.needs_input_grad)\n",
    "        x1_ = x1.div(lengthscale)\n",
    "        x2_ = x2.div(lengthscale)\n",
    "        unitless_sq_dist = sq_dist_func(x1_, x2_)\n",
    "        # clone because inplace operations will mess with what's saved for backward\n",
    "        unitless_sq_dist_ = unitless_sq_dist.clone() if needs_grad else unitless_sq_dist\n",
    "        covar_mat = unitless_sq_dist_.div_(-2.0).exp_()\n",
    "        if needs_grad:\n",
    "            d_output_d_input = unitless_sq_dist.mul_(covar_mat).div_(lengthscale)\n",
    "            ctx.save_for_backward(d_output_d_input)\n",
    "        return covar_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        d_output_d_input = ctx.saved_tensors[0]\n",
    "        lengthscale_grad = grad_output * d_output_d_input\n",
    "        return None, None, lengthscale_grad, None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
