{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import  DataLoader\n",
    "import numpy as np\n",
    "from gpytorch.kernels import Kernel\n",
    "import pickle\n",
    "\n",
    "from gpytorch.kernels import Kernel, RBFKernel\n",
    "\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from itertools import product\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset,TensorDataset\n",
    "\n",
    "from utils.HelperFunctions import *\n",
    "from utils.Models import *\n",
    "from utils.Kernels import *\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up logger. Training MOGP models in this research takes quite some time, especially if you are doing massive scale grid search or cross validation. The logger here is to assure even when training unexpectedly terminated with anomaly, records are still kept for later query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamHandler added\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger('The_Logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('result.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "error = logging.getLogger('error')\n",
    "error.setLevel(logging.DEBUG)\n",
    "error_fh=logging.FileHandler('error.log')\n",
    "error_fh.setLevel(logging.DEBUG)\n",
    "error_ch=logging.StreamHandler()\n",
    "error_fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
    "    error.addHandler(error_fh)\n",
    "    error.addHandler(error_ch)\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    print(\"StreamHandler added\")\n",
    "else:\n",
    "    print(\"StreamHandler already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'models':[Linear_Model_Of_Corregionalization,MultitaskGP],\n",
    "    'kernels': [K_MS_with_Feat_Scaling,K_MS,K_Alpha_Beta,RBFKernel],\n",
    "    'lrs': torch.log(torch.logspace(0.003,0.3,5)).tolist(),\n",
    "    'gammas':torch.linspace(0.2,0.8,5).tolist(),\n",
    "    'STEP_SIZEs':torch.linspace(10,100,5).tolist()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(274, 1084)\n",
      "(274, 10)\n",
      "(274, 1)\n",
      "Shape of X_train:torch.Size([246, 1084])\n",
      "Shape of X_D_train(domain information of X_train):torch.Size([246, 1084])\n",
      "Shape of y_train:torch.Size([246, 10])\n"
     ]
    }
   ],
   "source": [
    "X_path,y_path,X_domain_path=get_dataset_path(\"FULL_SHOTS\")\n",
    "(X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)=load_dataset(X_path,y_path,X_domain_path=X_domain_path)\n",
    "\n",
    "print(f\"Shape of X_train:{X_train_tensor.shape}\")\n",
    "print(f\"Shape of X_D_train(domain information of X_train):{X_train_tensor.shape}\")\n",
    "print(f\"Shape of y_train:{y_train_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating & configuring some global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMS_DOMAIN: 30.0\n",
      "torch.Size([246, 1085])\n",
      "torch.Size([28, 1085])\n"
     ]
    }
   ],
   "source": [
    "Global=config()\n",
    "\n",
    "\n",
    "Global.NUM_CONC=y_train_tensor.shape[1]\n",
    "\n",
    "Global.NUM_FEAT=X_train_tensor.shape[1]\n",
    "Global.NUM_DOMAIN_FEAT=X_D_train_tensor.shape[1]\n",
    "NUMS_DOMAIN, max_indices_row = torch.max(X_D_train_tensor, dim=0)\n",
    "NUMS_DOMAIN.add_(1)\n",
    "\n",
    "\n",
    "print(f'NUMS_DOMAIN: {NUMS_DOMAIN.item()}')\n",
    "Global.NUMS_DOMAIN=NUMS_DOMAIN.long()\n",
    "X_train_tensor = torch.cat((X_D_train_tensor, X_train_tensor), dim=1)\n",
    "X_test_tensor = torch.cat((X_D_test_tensor, X_test_tensor), dim=1)\n",
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Global.lr=0.1\n",
    "Global.gamma=0.5\n",
    "Global.STEP_SIZE=50\n",
    "Global.NUMS_DOMAIN_FEATURE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= [\n",
    "    # [MultitaskGP, RBFKernel, 0.18, 55, 0.8],\n",
    "    # [Linear_Model_Of_Corregionalization, RBFKernel, 0.1, 60, 0.8],\n",
    "    # [MultitaskGP, K_MS, 0.18, 55, 0.65],\n",
    "    [Linear_Model_Of_Corregionalization, K_Alpha_Beta,0.69, 33, 0.2],\n",
    "    # [MultitaskGP, K_MS_with_Feat_Scaling, 0.18, 55,0.65],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 21:23:47,407 - INFO - --------------------------------\n",
      "2024-09-16 21:23:47,408 - INFO - FOLD 1\n",
      "2024-09-16 21:23:47,420 - INFO - training starts for model <class 'utils.Models.Linear_Model_Of_Corregionalization'> with kernel <class 'utils.Kernels.K_Alpha_Beta'>; lr: 0.69; step_size:33; gamma:0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape: torch.Size([246, 1085])\n",
      "y_train_tensor shape: torch.Size([246, 10])\n",
      "X_train_tensor shape: torch.Size([28, 1085])\n",
      "y_train_tensor shape: torch.Size([28, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\Preliminary_Model\\utils\\Kernels.py:117: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\b\\abs_8f7uhuge1i\\croot\\pytorch-select_1717607507421\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3679.)\n",
      "  domain_mat_alpha=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m X_test_tensor,y_test_tensor\u001b[38;5;241m=\u001b[39mdataloader2tensor(val_loader)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m     md,n\u001b[38;5;241m=\u001b[39mrun_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m0\u001b[39m],kernel\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m1\u001b[39m],config\u001b[38;5;241m=\u001b[39mGlobal,logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m     33\u001b[0m     m\u001b[38;5;241m.\u001b[39mappend(md)\n\u001b[0;32m     34\u001b[0m     nlls\u001b[38;5;241m.\u001b[39mappend(n)\n",
      "File \u001b[1;32mc:\\Projects\\Preliminary_Model\\utils\\HelperFunctions.py:240\u001b[0m, in \u001b[0;36mrun_test\u001b[1;34m(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, model, kernel, config, logger)\u001b[0m\n\u001b[0;32m    238\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    239\u001b[0m output \u001b[38;5;241m=\u001b[39m m(X_train_tensor)\n\u001b[1;32m--> 240\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_train_tensor)\n\u001b[0;32m    241\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    243\u001b[0m this_loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:66\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactMarginalLogLikelihood can only operate on Gaussian random variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Determine output likelihood\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Remove NaN values if enabled\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39mvalue() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\likelihoods\\likelihood.py:76\u001b[0m, in \u001b[0;36m_Likelihood.__call__\u001b[1;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Marginal\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, MultivariateNormal):\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarginal(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Error\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLikelihoods expects a MultivariateNormal input to make marginal predictions, or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor for conditional predictions. Got a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\likelihoods\\multitask_gaussian_likelihood.py:299\u001b[0m, in \u001b[0;36mMultitaskGaussianLikelihood.marginal\u001b[1;34m(self, function_dist, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarginal\u001b[39m(\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m, function_dist: MultitaskMultivariateNormal, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    295\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MultitaskMultivariateNormal:\n\u001b[0;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    :return: Analytic marginal :math:`p(\\mathbf y)`.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmarginal(function_dist, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\likelihoods\\multitask_gaussian_likelihood.py:107\u001b[0m, in \u001b[0;36m_MultitaskGaussianLikelihoodBase.marginal\u001b[1;34m(self, function_dist, *params, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# ensure that sumKroneckerLT is actually called\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(covar, LazyEvaluatedKernelTensor):\n\u001b[1;32m--> 107\u001b[0m     covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[0;32m    109\u001b[0m covar_kron_lt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shaped_noise_covar(\n\u001b[0;32m    110\u001b[0m     mean\u001b[38;5;241m.\u001b[39mshape, add_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_global_noise, interleaved\u001b[38;5;241m=\u001b[39mfunction_dist\u001b[38;5;241m.\u001b[39m_interleaved\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar \u001b[38;5;241m+\u001b[39m covar_kron_lt\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[1;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(\n\u001b[0;32m    356\u001b[0m         x1,\n\u001b[0;32m    357\u001b[0m         x2,\n\u001b[0;32m    358\u001b[0m         diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    359\u001b[0m         last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dim_is_batch,\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 530\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Kernel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x1_, x2_, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\lcm_kernel.py:59\u001b[0m, in \u001b[0;36mLCMKernel.forward\u001b[1;34m(self, x1, x2, **params)\u001b[0m\n\u001b[0;32m     57\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mforward(x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module_list[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m---> 59\u001b[0m     res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward(x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\multitask_kernel.py:52\u001b[0m, in \u001b[0;36mMultitaskKernel.forward\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]):\n\u001b[0;32m     51\u001b[0m     covar_i \u001b[38;5;241m=\u001b[39m covar_i\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m*\u001b[39mx1\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m covar_x \u001b[38;5;241m=\u001b[39m to_linear_operator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_covar_module\u001b[38;5;241m.\u001b[39mforward(x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams))\n\u001b[0;32m     53\u001b[0m res \u001b[38;5;241m=\u001b[39m KroneckerProductLinearOperator(covar_x, covar_i)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mdiagonal(dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m diag \u001b[38;5;28;01melse\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Projects\\Preliminary_Model\\utils\\Kernels.py:114\u001b[0m, in \u001b[0;36mK_Alpha_Beta.forward\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    112\u001b[0m beta_reparameterized\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mNUMS_DOMAIN):\n\u001b[1;32m--> 114\u001b[0m     cov_mats_alpha\u001b[38;5;241m=\u001b[39mcov_mats_alpha\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels[i](x1, x2, diag\u001b[38;5;241m=\u001b[39mdiag, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\u001b[38;5;241m*\u001b[39malpha_reparameterized[i])\n\u001b[0;32m    115\u001b[0m     cov_mats_beta\u001b[38;5;241m=\u001b[39mcov_mats_beta\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels[i](x1, x2, diag\u001b[38;5;241m=\u001b[39mdiag, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\u001b[38;5;241m*\u001b[39mbeta_reparameterized[i])\n\u001b[0;32m    117\u001b[0m domain_mat_alpha\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mouter(x1_domain\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(x2_domain\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 530\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Kernel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x1_, x2_, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\rbf_kernel.py:80\u001b[0m, in \u001b[0;36mRBFKernel.forward\u001b[1;34m(self, x1, x2, diag, **params)\u001b[0m\n\u001b[0;32m     78\u001b[0m     x2_ \u001b[38;5;241m=\u001b[39m x2\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m postprocess_rbf(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_dist(x1_, x2_, square_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39mdiag, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams))\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RBFCovariance\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     81\u001b[0m     x1,\n\u001b[0;32m     82\u001b[0m     x2,\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengthscale,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x1, x2: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_dist(x1, x2, square_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams),\n\u001b[0;32m     85\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\functions\\rbf_covariance.py:17\u001b[0m, in \u001b[0;36mRBFCovariance.forward\u001b[1;34m(ctx, x1, x2, lengthscale, sq_dist_func)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# clone because inplace operations will mess with what's saved for backward\u001b[39;00m\n\u001b[0;32m     16\u001b[0m unitless_sq_dist_ \u001b[38;5;241m=\u001b[39m unitless_sq_dist\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mif\u001b[39;00m needs_grad \u001b[38;5;28;01melse\u001b[39;00m unitless_sq_dist\n\u001b[1;32m---> 17\u001b[0m covar_mat \u001b[38;5;241m=\u001b[39m unitless_sq_dist_\u001b[38;5;241m.\u001b[39mdiv_(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39mexp_()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_grad:\n\u001b[0;32m     19\u001b[0m     d_output_d_input \u001b[38;5;241m=\u001b[39m unitless_sq_dist\u001b[38;5;241m.\u001b[39mmul_(covar_mat)\u001b[38;5;241m.\u001b[39mdiv_(lengthscale)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = torch.cat((X_train_tensor, X_test_tensor), dim=0)\n",
    "y = torch.cat((y_train_tensor, y_test_tensor), dim=0)\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# 定义 KFold，n_splits=10 表示 10-fold 交叉验证\n",
    "k_folds = 10\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 用于存储每个 fold 的结果\n",
    "fold_results = {}\n",
    "m=[]\n",
    "nlls=[]\n",
    "for each_param in params:\n",
    "        Global.lr=each_param[2]\n",
    "        Global.STEP_SIZE=each_param[3]\n",
    "        Global.gamma=each_param[4]\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "            logger.info('--------------------------------')\n",
    "            logger.info(f'FOLD {fold+1}')\n",
    "\n",
    "            \n",
    "            # 创建训练和验证数据集\n",
    "            train_subset = Subset(dataset, train_idx)\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "            \n",
    "            # 创建数据加载器\n",
    "            train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "            X_train_tensor,y_train_tensor=dataloader2tensor(train_loader)\n",
    "            X_test_tensor,y_test_tensor=dataloader2tensor(val_loader)\n",
    "            try:\n",
    "                md,n=run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model=each_param[0],kernel=each_param[1],config=Global,logger=logger)\n",
    "                m.append(md)\n",
    "                nlls.append(n)\n",
    "            except Exception as e:\n",
    "                error.error(f\"Model:{str(each_param[0])}; Kernel:{str(each_param[1])}; lr:{str(Global.lr)};STEP_SIZE:{str(Global.STEP_SIZE)}; {e}\")\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'models':[Linear_Model_Of_Corregionalization],\n",
    "    'kernels': [RBFKernel],\n",
    "    'lrs': [0.69 ],\n",
    "    'gammas':[0.5],\n",
    "    'STEP_SIZEs':[33]\n",
    "}\n",
    "\n",
    "grid_search(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,param_grid,Global)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
