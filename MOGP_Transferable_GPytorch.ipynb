{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gpytorch.kernels import Kernel\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.means import Mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gpytorch.kernels import Kernel, RBFKernel,ScaleKernel\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.settings import cholesky_jitter\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from itertools import product\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamHandler added\n",
      "GPU is not available!\n"
     ]
    }
   ],
   "source": [
    "# 创建一个logger\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建文件处理器，写入日志文件\n",
    "fh = logging.FileHandler('app.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "\n",
    "# 设置日志格式\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# 将处理器添加到logger中\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    print(\"StreamHandler added\")\n",
    "else:\n",
    "    print(\"StreamHandler already exists\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available!\")\n",
    "USE_TOY_DATASET=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(X_path,y_path,X_domain_path=None,do_standardisation=False,test_size=0.1,random_state=42):\n",
    "    X_df=None\n",
    "    X_domain_info=None\n",
    "    y_df=None\n",
    "    with open(X_path, 'rb') as f:\n",
    "        X_df = pickle.load(f)\n",
    "    with open(y_path, 'rb') as f:\n",
    "        y_df = pickle.load(f)\n",
    "    print(X_df.shape)\n",
    "    print(y_df.shape)\n",
    "    if X_domain_path!=None:\n",
    "        with open(X_domain_path, 'rb') as f:\n",
    "            X_domain_info = pickle.load(f)\n",
    "        print(X_domain_info.shape)\n",
    "        X_train, X_test, y_train, y_test,X_D_train,X_D_test = train_test_split(X_df, y_df,X_domain_info, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test= train_test_split(X_df, y_df, test_size=test_size, random_state=random_state)\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train = np.array(X_D_train, dtype=np.float32)\n",
    "        X_D_test = np.array(X_D_test, dtype=np.float32)\n",
    "\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "    if do_standardisation:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train_tensor = torch.tensor(X_D_train)\n",
    "        X_D_test_tensor = torch.tensor(X_D_test)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test)\n",
    "    y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "    if X_domain_path!=None: \n",
    "        return (X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)\n",
    "    else:\n",
    "        return (X_train_tensor,y_train_tensor),(X_test_tensor,y_test_tensor)\n",
    "\n",
    "def sigmoid_4_param(x, x0, L, k, d):\n",
    "    \"\"\" Comparing with Dennis Wang's sigmoid:\n",
    "    x0 -  p - position, correlation with IC50 or EC50\n",
    "        bounds [0, 1]\n",
    "    L = 1 in Dennis Wang's sigmoid, protect from devision by zero if x is too small \n",
    "        L<1 inverted sigmoid, l=100 - lower upper and lower boundso sigmpoid on y axis (y= [0.1, 0.11])\n",
    "        bounds [0.8, 10]\n",
    "    k = -1/s (s -shape parameter)  default = -10 k=0 straight line, k<0 sigmoid around k=-10\n",
    "        bounds [1, -100]\n",
    "    d - determines the vertical position of the sigmoid - shift on y axis - better fitting then Dennis Wang's sigmoid\n",
    "         bounds [0, 0.9]\n",
    "    parameters_bound ((0, 0.8, -100, 0), (1, 10, 1, 0.9))\n",
    "    \"\"\"\n",
    "    return ( 1/ (L + np.exp(-k*(x-x0))) + d)\n",
    "\n",
    "def overwrite_to_test():\n",
    "    VAR=0.09\n",
    "    AMP=1.0\n",
    "    train_size=100\n",
    "\n",
    "    X_sin=torch.linspace(0, 1, train_size)\n",
    "    X_cos=torch.linspace(0, 1, train_size)\n",
    "    X_sig=torch.linspace(0, 1, train_size)\n",
    "    y_sin = AMP*torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos = AMP*torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig = AMP*(torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    # Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "    X_train_tensor = torch.linspace(0, 1, train_size)\n",
    "    # True function is sin(2*pi*x) with Gaussian noise\n",
    "\n",
    "    num_conc=1\n",
    "    num_feat=1\n",
    "    nums_domain=torch.Tensor([3])\n",
    "    num_data=train_size\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "    y_sin = y_sin.unsqueeze(1)\n",
    "    y_cos = y_cos.unsqueeze(1)\n",
    "    y_sig = y_sig.unsqueeze(1)\n",
    "\n",
    "    X_sin_domain=torch.zeros(train_size,1)\n",
    "    X_cos_domain=torch.ones(train_size,1)\n",
    "    X_sig_domain=torch.ones(train_size,1)*2\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "    X_train_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_train_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    for i in range(3):\n",
    "        ax[i].plot(X_train_tensor[i*train_size:(i+1)*train_size,1:].squeeze().numpy(),y_train_tensor[i*train_size:(i+1)*train_size], 'k*')\n",
    "        axis=X_train_tensor[i*train_size:(i+1)*train_size,1:].flatten().numpy()\n",
    "        ax[i].set_ylim([-3, 3])\n",
    "        ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "\n",
    "    # indices = torch.randperm(X_train_tensor.size(0))\n",
    "    # X_train_tensor = X_train_tensor[indices]\n",
    "    # y_train_tensor = y_train_tensor[indices]\n",
    "\n",
    "    test_size=50\n",
    "    X_sin=torch.linspace(0, 1, test_size)\n",
    "    X_cos=torch.linspace(0, 1, test_size)\n",
    "    X_sig=torch.linspace(0, 1, test_size)\n",
    "\n",
    "    X_sin_domain=torch.zeros(test_size,1)\n",
    "    X_cos_domain=torch.ones(test_size,1)\n",
    "    X_sig_domain=torch.ones(test_size,1)*2\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "\n",
    "    y_sin =AMP* torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos =AMP* torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig =AMP* (torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "\n",
    "    # X_test_tensor =X_sin_cat\n",
    "    # y_test_tensor =y_sin\n",
    "    \n",
    "    X_test_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_test_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    # X_test_tensor = torch.cat((X_sin_cat, X_cos_cat), dim=0)\n",
    "    # y_test_tensor = torch.cat((y_sin, y_cos), dim=0)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    NUMS_DOMAIN=None\n",
    "    NUMS_DOMAIN_FEATURE=None\n",
    "    NUMS_DOMAIN_AS_INT=None\n",
    "    NUM_CONC=None\n",
    "    STEP_SIZE=None\n",
    "    lr=None\n",
    "    gamma=None\n",
    "    def __init__(self, NUMS_DOMAIN=None,\n",
    "                 NUMS_DOMAIN_FEATURE=None,\n",
    "                 NUMS_DOMAIN_AS_INT=None,\n",
    "                 NUM_CONC=None,\n",
    "                 STEP_SIZE=None,\n",
    "                 lr=None,\n",
    "                 gamma=None):\n",
    "        self.NUMS_DOMAIN=NUMS_DOMAIN\n",
    "        self.NUMS_DOMAIN_FEATURE=NUMS_DOMAIN_FEATURE\n",
    "        self.NUMS_DOMAIN_AS_INT=NUMS_DOMAIN_AS_INT\n",
    "        self.NUM_CONC=NUM_CONC\n",
    "        self.STEP_SIZE=STEP_SIZE\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model,kernel,config):\n",
    "    logger.info(f'training starts for model {str(model)} with kernel {str(kernel)}; lr: {config.lr}; step_size:{config.STEP_SIZE}; gamma:{config.gamma}')\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=config.NUM_CONC)\n",
    "\n",
    "    m = model(X_train_tensor, y_train_tensor, likelihood,kernel,config)\n",
    "\n",
    "    training_iterations = 500\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    m.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=config.lr)\n",
    "    STEP_SIZE=config.STEP_SIZE\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=config.gamma)\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, m)\n",
    "    last_loss=1\n",
    "    avg_loss=0\n",
    "    this_loss=0\n",
    "    for i in range(training_iterations):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            output = m(X_train_tensor)\n",
    "            loss = -mll(output, y_train_tensor)\n",
    "            loss.backward()\n",
    "                \n",
    "            this_loss=loss.item()\n",
    "            avg_loss+=this_loss\n",
    "            optimizer.step()  # 更新参数\n",
    "            scheduler.step()  # 更新学习率\n",
    "            \n",
    "            if i%STEP_SIZE==STEP_SIZE-1:\n",
    "                logger.debug('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "                avg_loss=avg_loss/STEP_SIZE\n",
    "                if abs((this_loss-avg_loss)/avg_loss)<0.001:\n",
    "                    logger.info(f'Early cut off at epoch {i} with loss of {this_loss }')\n",
    "                    break\n",
    "                        \n",
    "                avg_loss=0\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"发生了一个异常: {e}\")\n",
    "            continue\n",
    "        \n",
    "    m.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        distribution = likelihood(m(X_test_tensor))\n",
    "        mean = distribution.mean\n",
    "        lower, upper = distribution.confidence_region()\n",
    "    nll = -torch.distributions.Normal(mean, distribution.variance.sqrt()).log_prob(y_test_tensor).mean().item()\n",
    "\n",
    "    rmse=torch.sqrt(torch.tensor(mean_squared_error(y_test_tensor.numpy(), mean.numpy()))).item() \n",
    "    nmse = rmse / torch.var(y_test_tensor).item()\n",
    "\n",
    "    logger.info(f'NLL: {nll:.4f}; RMSE: {rmse:.4f}; NMSE: {nmse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Exponential Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_Exponential_Squared(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.height_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN)))\n",
    "        self.length_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN))) \n",
    "        self.register_parameter(name='height_scales', parameter=self.height_scales)\n",
    "        self.register_parameter(name='length_scales', parameter=self.length_scales)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        # x1_domain   =   x1[:, :num_domain_feat]\n",
    "        # x1_genetics =   x1[:, num_domain_feat:]\n",
    "        # x2_domain   =   x2[:, :num_domain_feat]\n",
    "        # x2_genetics =   x2[:, num_domain_feat:]\n",
    "\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetics =   x1\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_genetics =   x2\n",
    "\n",
    "        dist_mat=self.covar_dist(x1_genetics, x2_genetics, square_dist=True, diag=diag, **params)\n",
    "\n",
    "        height_scales_parsed_1 = self.height_scales[x1_domain.long()].flatten()\n",
    "        height_scales_parsed_2 = self.height_scales[x2_domain.long()].flatten()\n",
    "        length_scales_parsed_1 = self.length_scales[x1_domain.long()].flatten()\n",
    "        length_scales_parsed_2 = self.length_scales[x2_domain.long()].flatten()\n",
    "\n",
    "        part_L1L2T=torch.sqrt(torch.outer(length_scales_parsed_1*length_scales_parsed_1,length_scales_parsed_2.T*length_scales_parsed_2.T))\n",
    "        part_L1L1T=length_scales_parsed_1*length_scales_parsed_1\n",
    "        part_L2L2T=length_scales_parsed_2*length_scales_parsed_2\n",
    "        part_L1sqrL2sqr=torch.outer(part_L1L1T,torch.ones_like(part_L2L2T).T)+torch.outer(torch.ones_like(part_L1L1T),part_L2L2T)\n",
    "\n",
    "        part_1=torch.outer(height_scales_parsed_1,height_scales_parsed_2.T)\n",
    "        part_2=torch.sqrt(2*part_L1L2T/part_L1sqrL2sqr)\n",
    "        part_3=torch.exp(-dist_mat/part_L1sqrL2sqr)\n",
    "\n",
    "        result=part_1*part_2*part_3\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class K_MS(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.domain_coefficient=torch.nn.Parameter(torch.rand((config.NUMS_DOMAIN))*2-1)\n",
    "        self.K_ES=Kernel_Exponential_Squared(config)\n",
    "        self.register_parameter(name='domain_coefficient', parameter=self.domain_coefficient)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "\n",
    "        domain_coefficient_parsed_1=self.domain_coefficient[x1_domain.long()].flatten()\n",
    "        domain_coefficient_parsed_2=self.domain_coefficient[x2_domain.long()].flatten()\n",
    "        domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
    "\n",
    "        domain_mat=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat == 1.0)\n",
    "\n",
    "        # 等于1的位置置0\n",
    "        domain_mat[mask] = 0.0\n",
    "\n",
    "        # 不等于1的位置置1\n",
    "        domain_mat[~mask] = 1.0\n",
    "        # print(domain_scaler)\n",
    "\n",
    "        \n",
    "        base_cov=self.K_ES(x1,x2,diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
    "\n",
    "        final_scaler=(domain_scaler-1.0)*domain_mat+1.0\n",
    "        return base_cov*final_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_Alpha_Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Alpha_Beta(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernels=[]\n",
    "        for _ in range(config.NUMS_DOMAIN):\n",
    "            self.kernels.append(RBFKernel())\n",
    "        self.config=config\n",
    "        self.alpha= torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "        self.beta=torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):#\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_num_data=x1_domain.shape[0]\n",
    "        x2_num_data=x2_domain.shape[0]\n",
    "        cov_mats_alpha=torch.zeros([x1_num_data,x2_num_data])\n",
    "        cov_mats_beta=torch.zeros([x1_num_data,x2_num_data])\n",
    "\n",
    "        alpha_reparameterized=torch.sqrt(self.alpha*self.alpha)\n",
    "        beta_reparameterized=torch.sqrt(self.beta*self.beta)\n",
    "        for i in range(self.config.NUMS_DOMAIN):\n",
    "            cov_mats_alpha=cov_mats_alpha.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*alpha_reparameterized[i])\n",
    "            cov_mats_beta=cov_mats_beta.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*beta_reparameterized[i])\n",
    "\n",
    "        domain_mat_alpha=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        domain_mat_beta=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat_alpha == 1.0)\n",
    "        domain_mat_alpha[mask] = 0.0\n",
    "        domain_mat_alpha[~mask] = 1.0\n",
    "        domain_mat_beta[mask] = 1.0\n",
    "        domain_mat_beta[~mask] = 0.0\n",
    "\n",
    "        \n",
    "        result=cov_mats_alpha*domain_mat_alpha+cov_mats_beta*domain_mat_beta\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferable quadriple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quadriple_Kernel_Exponential_Squared(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernel_mu = Kernel_Exponential_Squared(config)\n",
    "        self.kernel_met = Kernel_Exponential_Squared(config)\n",
    "        self.kernel_cn = Kernel_Exponential_Squared(config)\n",
    "        # self.kernel_dc = Kernel_Exponential_Squared()\n",
    "        self.filter_mu,self.filter_met,self.filter_cn,self.filter_dc=config.filters\n",
    "        initial_value = torch.ones(config.NUMS_DOMAIN)\n",
    "        self.domain_relateness = torch.nn.Parameter(initial_value)\n",
    "        self.config=config\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetics =   x1[:, self.config.NUMS_DOMAIN_FEATURE:]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_genetics =   x2[:, self.config.NUMS_DOMAIN_FEATURE:]\n",
    "        # # Apply the base kernel\n",
    "        filter_mu=self.filter_mu\n",
    "        filter_met=self.filter_met\n",
    "        filter_cn=self.filter_cn\n",
    "        filter_dc=self.filter_dc\n",
    "        \n",
    "        x1_mu   =   x1_genetics @ filter_mu\n",
    "        x1_met  =   x1_genetics @ filter_met\n",
    "        x1_cn   =   x1_genetics @ filter_cn\n",
    "        # x1_dc   =   x1_genetics @ filter_dc\n",
    "\n",
    "        x2_mu   =   x2_genetics @ filter_mu\n",
    "        x2_met  =   x2_genetics @ filter_met\n",
    "        x2_cn   =   x2_genetics @ filter_cn\n",
    "        # x2_dc   =   x2_genetics @ filter_dc\n",
    "\n",
    "        domain_relateness_multiplier=self.domain_relateness\n",
    "        # 获取领域系数\n",
    "        domain_factors1 = torch.tanh(self.domain_relateness[x1_domain.long()])\n",
    "        domain_factors2 = torch.tanh(self.domain_relateness[x2_domain.long()])\n",
    "\n",
    "        base_cov=self.kernel_mu(x1_mu,x2_mu)*self.kernel_met(x1_met,x2_met)*self.kernel_cn(x1_cn,x2_cn)* 1#self.kernel_dc(x1_dc,x2_dc)\n",
    "        if diag:\n",
    "            adjusted_cov = base_cov * (domain_factors1 * domain_factors2).squeeze()\n",
    "        else:\n",
    "            factor= (domain_factors1 @ domain_factors2.T)\n",
    "            adjusted_cov = base_cov *factor\n",
    "        return adjusted_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class K_MS_Quadriple(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.domain_coefficient=torch.nn.Parameter(torch.rand((config.NUMS_DOMAIN))*2-1)\n",
    "        self.K_ES=Quadriple_Kernel_Exponential_Squared(config)\n",
    "        self.register_parameter(name='domain_coefficient', parameter=self.domain_coefficient)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "\n",
    "        domain_coefficient_parsed_1=self.domain_coefficient[x1_domain.long()].flatten()\n",
    "        domain_coefficient_parsed_2=self.domain_coefficient[x2_domain.long()].flatten()\n",
    "        domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
    "\n",
    "        domain_mat=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat == 1.0)\n",
    "\n",
    "        # 等于1的位置置0\n",
    "        domain_mat[mask] = 0.0\n",
    "\n",
    "        # 不等于1的位置置1\n",
    "        domain_mat[~mask] = 1.0\n",
    "        # print(domain_scaler)\n",
    "\n",
    "        \n",
    "        base_cov=self.K_ES(x1,x2,diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
    "\n",
    "        final_scaler=(domain_scaler-1.0)*domain_mat+1.0\n",
    "        return base_cov*final_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model of corregionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Model_Of_Corregionalization(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(Linear_Model_Of_Corregionalization, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        if kernel==RBFKernel:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel())\n",
    "        else:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel(config))\n",
    "        self.covar_module =gpytorch.kernels.LCMKernel(\n",
    "            kernels, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "\n",
    "        logger.debug(\"Completed\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGP(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(MultitaskGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        \n",
    "        if kernel==RBFKernel:\n",
    "            kern=kernel()\n",
    "        else:\n",
    "            kern=kernel(config)\n",
    "                \n",
    "        self.covar_module =gpytorch.kernels.MultitaskKernel(\n",
    "            kern, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "        #     self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "        #         K_MS(),  # 2 latent GP's\n",
    "\n",
    "        #     num_tasks=num_conc, rank=num_conc-1\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 1084)\n",
      "(62, 10)\n",
      "(62, 1)\n",
      "torch.Size([55, 1084])\n",
      "torch.Size([55, 1])\n",
      "torch.Size([55, 10])\n",
      "13.0\n",
      "torch.Size([55, 1085])\n",
      "torch.Size([7, 1085])\n",
      "MUT features:310.0torch.Size([1084, 1084])\n",
      "Met features:338.0torch.Size([1084, 1084])\n",
      "CNA features:425.0torch.Size([1084, 1084])\n",
      "DC features:0.0torch.Size([1084, 1084])\n"
     ]
    }
   ],
   "source": [
    "# X_path=\"data/X_df_toy.pkl\"\n",
    "# y_path=\"data/y_df_toy.pkl\"\n",
    "\n",
    "# X_path=\"data/X_df_Shikonin.pkl\"\n",
    "# y_path=\"data/y_df_Shikonin.pkl\"\n",
    "# X_domain_path=\"data/X_domain_info_Shikonin.pkl\"\n",
    "\n",
    "Global=config()\n",
    "\n",
    "# X_path=\"data/X_df_2_shots.pkl\"\n",
    "# y_path=\"data/y_df_2_shots.pkl\"\n",
    "# X_domain_path=\"data/X_domain_info_2_shots.pkl\"\n",
    "\n",
    "# X_path=\"data/2_shots_4_params_sigmoid/X_df.pkl\"\n",
    "# y_path=\"data/2_shots_4_params_sigmoid/y_df.pkl\"\n",
    "# X_domain_path=\"data/2_shots_4_params_sigmoid/X_domain_info.pkl\"\n",
    "\n",
    "X_path=\"data/8_shots/X_df.pkl\"\n",
    "y_path=\"data/8_shots/y_df.pkl\"\n",
    "X_domain_path=\"data/8_shots/X_domain_info.pkl\"\n",
    "(X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)=load_dataset(X_path,y_path,do_standardisation=True,X_domain_path=X_domain_path)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(X_D_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "Global.NUM_CONC=y_train_tensor.shape[1]\n",
    "\n",
    "Global.NUM_FEAT=X_train_tensor.shape[1]\n",
    "Global.NUM_DOMAIN_FEAT=X_D_train_tensor.shape[1]\n",
    "NUMS_DOMAIN, max_indices_row = torch.max(X_D_train_tensor, dim=0)\n",
    "NUMS_DOMAIN.add_(1)\n",
    "\n",
    "\n",
    "print(NUMS_DOMAIN.item())\n",
    "Global.NUMS_DOMAIN=NUMS_DOMAIN.long()\n",
    "X_train_tensor = torch.cat((X_D_train_tensor, X_train_tensor), dim=1)\n",
    "X_test_tensor = torch.cat((X_D_test_tensor, X_test_tensor), dim=1)\n",
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n",
    "\n",
    "with open(X_path, 'rb') as f:\n",
    "    X_df = pickle.load(f)\n",
    "mask_bool_met = X_df.columns.str.contains(\"HypMET\")\n",
    "mask_bool_mut = X_df.columns.str.contains(\"mut\")\n",
    "mask_bool_cna = X_df.columns.str.contains(\"cna\")\n",
    "mask_bool_dc  = X_df.columns.str.contains(\"dc\")\n",
    "\n",
    "mask_float_met = np.array(mask_bool_met, dtype=np.float32) \n",
    "mask_float_mut = np.array(mask_bool_mut, dtype=np.float32)\n",
    "mask_float_cna = np.array(mask_bool_cna, dtype=np.float32)\n",
    "mask_float_dc  = np.array(mask_bool_dc, dtype=np.float32)\n",
    "\n",
    "diag_matrix_met = torch.tensor(np.diag(mask_float_met))\n",
    "diag_matrix_mut = torch.tensor(np.diag(mask_float_mut))\n",
    "diag_matrix_cna = torch.tensor(np.diag(mask_float_cna))\n",
    "diag_matrix_dc  = torch.tensor(np.diag(mask_float_dc))\n",
    "filters=(diag_matrix_mut,diag_matrix_met,diag_matrix_cna,diag_matrix_dc)\n",
    "print(\"MUT features:\"+str(np.sum(mask_float_mut))+str(diag_matrix_met.shape))\n",
    "print(\"Met features:\"+str(np.sum(mask_float_met))+str(diag_matrix_met.shape))\n",
    "print(\"CNA features:\"+str(np.sum(mask_float_cna))+str(diag_matrix_met.shape))\n",
    "print(\"DC features:\"+str(np.sum(mask_float_dc))+str(diag_matrix_met.shape))\n",
    "Global.filters=filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Global.lr=0.1\n",
    "Global.gamma=0.5\n",
    "Global.STEP_SIZE=50\n",
    "Global.NUMS_DOMAIN_FEATURE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'models':[Linear_Model_Of_Corregionalization,MultitaskGP],\n",
    "    'kernels': [K_MS_Quadriple],\n",
    "    'lrs': torch.log(torch.logspace(0.003,0.3,5)).tolist(),\n",
    "    'gammas':torch.linspace(0.2,0.8,5).tolist(),\n",
    "    'STEP_SIZEs':torch.linspace(10,100,5).tolist()\n",
    "}\n",
    "\n",
    "param_combinations = list(product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-19 17:17:31,883 - INFO - training starts for model <class '__main__.Linear_Model_Of_Corregionalization'> with kernel <class '__main__.K_MS_Quadriple'>; lr: 0.1; step_size:50; gamma:0.5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Kernel_Exponential_Squared.__init__() missing 1 required positional argument: 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model\u001b[38;5;241m=\u001b[39mLinear_Model_Of_Corregionalization,kernel\u001b[38;5;241m=\u001b[39mK_MS_Quadriple,config\u001b[38;5;241m=\u001b[39mGlobal)\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mrun_test\u001b[1;34m(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, model, kernel, config)\u001b[0m\n\u001b[0;32m      2\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining starts for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(model)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kernel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(kernel)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; step_size:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mSTEP_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; gamma:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m gpytorch\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mMultitaskGaussianLikelihood(num_tasks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mNUM_CONC)\n\u001b[1;32m----> 5\u001b[0m m \u001b[38;5;241m=\u001b[39m model(X_train_tensor, y_train_tensor, likelihood,kernel,config)\n\u001b[0;32m      7\u001b[0m training_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Find optimal model hyperparameters\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mLinear_Model_Of_Corregionalization.__init__\u001b[1;34m(self, train_x, train_y, likelihood, kernel, config)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mNUM_CONC):\n\u001b[1;32m---> 14\u001b[0m         kernels\u001b[38;5;241m.\u001b[39mappend(kernel(config))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module \u001b[38;5;241m=\u001b[39mgpytorch\u001b[38;5;241m.\u001b[39mkernels\u001b[38;5;241m.\u001b[39mLCMKernel(\n\u001b[0;32m     16\u001b[0m     kernels, num_tasks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mNUM_CONC, rank\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mNUM_CONC\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m, in \u001b[0;36mK_MS_Quadriple.__init__\u001b[1;34m(self, config, **kwargs)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_coefficient\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mrand((config\u001b[38;5;241m.\u001b[39mNUMS_DOMAIN))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK_ES\u001b[38;5;241m=\u001b[39mQuadriple_Kernel_Exponential_Squared(config)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdomain_coefficient\u001b[39m\u001b[38;5;124m'\u001b[39m, parameter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_coefficient)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m=\u001b[39mconfig\n",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m, in \u001b[0;36mQuadriple_Kernel_Exponential_Squared.__init__\u001b[1;34m(self, config, **kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_mu \u001b[38;5;241m=\u001b[39m Kernel_Exponential_Squared()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_met \u001b[38;5;241m=\u001b[39m Kernel_Exponential_Squared()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_cn \u001b[38;5;241m=\u001b[39m Kernel_Exponential_Squared()\n",
      "\u001b[1;31mTypeError\u001b[0m: Kernel_Exponential_Squared.__init__() missing 1 required positional argument: 'config'"
     ]
    }
   ],
   "source": [
    "run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model=Linear_Model_Of_Corregionalization,kernel=K_MS_Quadriple,config=Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 14:12:29,495 - INFO - training starts for model <class '__main__.Linear_Model_Of_Corregionalization'> with kernel <class '__main__.K_MS'>; lr: 0.006907748989760876; step_size:10.0; gamma:0.20000000298023224\n",
      "2024-08-15 14:12:29,515 - DEBUG - Completed\n",
      "C:\\Users\\25858\\AppData\\Local\\Temp\\ipykernel_41544\\3416435673.py:14: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\b\\abs_8f7uhuge1i\\croot\\pytorch-select_1717607507421\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3679.)\n",
      "  domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
      "2024-08-15 14:12:31,360 - DEBUG - Iter 10/500 - Loss: 3.148\n",
      "2024-08-15 14:12:31,900 - DEBUG - Iter 20/500 - Loss: 3.111\n",
      "2024-08-15 14:12:32,477 - DEBUG - Iter 30/500 - Loss: 3.104\n",
      "2024-08-15 14:12:32,478 - INFO - Early cut off at epoch 29 with loss of 3.104165554046631\n",
      "2024-08-15 14:12:32,549 - INFO - NLL: 3.1221; RMSE: 0.5903; NMSE: 3.0278\n",
      "2024-08-15 14:12:32,550 - INFO - training starts for model <class '__main__.Linear_Model_Of_Corregionalization'> with kernel <class '__main__.K_MS'>; lr: 0.006907748989760876; step_size:32.5; gamma:0.20000000298023224\n",
      "2024-08-15 14:12:32,555 - DEBUG - Completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m Global\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      4\u001b[0m Global\u001b[38;5;241m.\u001b[39mSTEP_SIZE\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m0\u001b[39m],kernel\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m1\u001b[39m],config\u001b[38;5;241m=\u001b[39mGlobal)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mrun_test\u001b[1;34m(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, model, kernel, config)\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m output \u001b[38;5;241m=\u001b[39m m(X_train_tensor)\n\u001b[1;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_train_tensor)\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m this_loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:82\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN observation policy \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported by ExactMarginalLogLikelihood!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Get the log prob of the marginal distribution\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m res \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlog_prob(target)\n\u001b[0;32m     83\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_other_terms(res, params)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Scale by the amount of data we have\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\distributions\\multitask_multivariate_normal.py:217\u001b[0m, in \u001b[0;36mMultitaskMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    215\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    216\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(new_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mlog_prob(value\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mvalue\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:193\u001b[0m, in \u001b[0;36mMultivariateNormal.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Get log determininant and first part of quadratic form\u001b[39;00m\n\u001b[0;32m    192\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[1;32m--> 193\u001b[0m inv_quad, logdet \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39minv_quad_logdet(inv_quad_rhs\u001b[38;5;241m=\u001b[39mdiff\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), logdet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    195\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([inv_quad, logdet, diff\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)])\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1701\u001b[0m, in \u001b[0;36mLinearOperator.inv_quad_logdet\u001b[1;34m(self, inv_quad_rhs, logdet, reduce_inv_quad)\u001b[0m\n\u001b[0;32m   1699\u001b[0m             will_need_cholesky \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m will_need_cholesky:\n\u001b[1;32m-> 1701\u001b[0m         cholesky \u001b[38;5;241m=\u001b[39m CholLinearOperator(TriangularLinearOperator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcholesky()))\n\u001b[0;32m   1702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cholesky\u001b[38;5;241m.\u001b[39minv_quad_logdet(\n\u001b[0;32m   1703\u001b[0m         inv_quad_rhs\u001b[38;5;241m=\u001b[39minv_quad_rhs,\n\u001b[0;32m   1704\u001b[0m         logdet\u001b[38;5;241m=\u001b[39mlogdet,\n\u001b[0;32m   1705\u001b[0m         reduce_inv_quad\u001b[38;5;241m=\u001b[39mreduce_inv_quad,\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1708\u001b[0m \u001b[38;5;66;03m# Short circuit to inv_quad function if we're not computing logdet\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1303\u001b[0m, in \u001b[0;36mLinearOperator.cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;129m@_implements\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky)\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m], upper: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch N N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# returns TriangularLinearOperator\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m \u001b[38;5;124;03m    Cholesky-factorizes the LinearOperator.\u001b[39;00m\n\u001b[0;32m   1299\u001b[0m \n\u001b[0;32m   1300\u001b[0m \u001b[38;5;124;03m    :param upper: Upper triangular or lower triangular factor (default: False).\u001b[39;00m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    :return: Cholesky factor (lower or upper triangular)\u001b[39;00m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1303\u001b[0m     chol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cholesky(upper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m upper:\n\u001b[0;32m   1305\u001b[0m         chol \u001b[38;5;241m=\u001b[39m chol\u001b[38;5;241m.\u001b[39m_transpose_nonbatch()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:515\u001b[0m, in \u001b[0;36mLinearOperator._cholesky\u001b[1;34m(self, upper)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(sub_mat, KeOpsLinearOperator) \u001b[38;5;28;01mfor\u001b[39;00m sub_mat \u001b[38;5;129;01min\u001b[39;00m evaluated_kern_mat\u001b[38;5;241m.\u001b[39m_args):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run Cholesky with KeOps: it will either be really slow or not work.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 515\u001b[0m evaluated_mat \u001b[38;5;241m=\u001b[39m evaluated_kern_mat\u001b[38;5;241m.\u001b[39mto_dense()\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# if the tensor is a scalar, we can just take the square root\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluated_mat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py:80\u001b[0m, in \u001b[0;36mSumLinearOperator.to_dense\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(linear_op\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py:80\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(linear_op\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py:80\u001b[0m, in \u001b[0;36mSumLinearOperator.to_dense\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(linear_op\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py:80\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;129m@cached\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28msum\u001b[39m(linear_op\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:2601\u001b[0m, in \u001b[0;36mLinearOperator.to_dense\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2599\u001b[0m     eye \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(num_cols, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2600\u001b[0m     eye \u001b[38;5;241m=\u001b[39m eye\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_shape, num_cols, num_cols)\n\u001b[1;32m-> 2601\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatmul(eye)\n\u001b[0;32m   2602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1831\u001b[0m, in \u001b[0;36mLinearOperator.matmul\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatmul_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatmulLinearOperator\n\u001b[0;32m   1829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatmulLinearOperator(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m-> 1831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Matmul\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_tree(), other, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation())\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    606\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\functions\\_matmul.py:21\u001b[0m, in \u001b[0;36mMatmul.forward\u001b[1;34m(ctx, representation_tree, rhs, *matrix_args)\u001b[0m\n\u001b[0;32m     18\u001b[0m     is_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m linear_op \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mrepresentation_tree(\u001b[38;5;241m*\u001b[39mmatrix_args)\n\u001b[1;32m---> 21\u001b[0m res \u001b[38;5;241m=\u001b[39m linear_op\u001b[38;5;241m.\u001b[39m_matmul(rhs)\n\u001b[0;32m     23\u001b[0m to_save \u001b[38;5;241m=\u001b[39m [orig_rhs] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(matrix_args)\n\u001b[0;32m     24\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mto_save)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\kronecker_product_linear_operator.py:270\u001b[0m, in \u001b[0;36mKroneckerProductLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vec:\n\u001b[0;32m    268\u001b[0m     rhs \u001b[38;5;241m=\u001b[39m rhs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 270\u001b[0m res \u001b[38;5;241m=\u001b[39m _matmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vec:\n\u001b[0;32m    273\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\kronecker_product_linear_operator.py:42\u001b[0m, in \u001b[0;36m_matmul\u001b[1;34m(linear_ops, kp_shape, rhs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m linear_ops:\n\u001b[0;32m     41\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput_batch_shape, linear_op\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m     factor \u001b[38;5;241m=\u001b[39m linear_op\u001b[38;5;241m.\u001b[39m_matmul(res)\n\u001b[0;32m     43\u001b[0m     factor \u001b[38;5;241m=\u001b[39m factor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39moutput_batch_shape, linear_op\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_cols)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     44\u001b[0m     res \u001b[38;5;241m=\u001b[39m factor\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39moutput_batch_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, num_cols)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\linear_operator\\operators\\dense_linear_operator.py:65\u001b[0m, in \u001b[0;36mDenseLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     63\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor, rhs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for each_param in param_combinations:\n",
    "    Global.lr=each_param[2]\n",
    "    Global.gamma=each_param[3]\n",
    "    Global.STEP_SIZE=each_param[4]\n",
    "    run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model=each_param[0],kernel=each_param[1],config=Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TOY_DATASET == True:\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(X_test_tensor))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize plot\n",
    "        f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        mean=observed_pred.mean.numpy()\n",
    "        for i in range(3):\n",
    "            ax[i].plot(X_test_tensor[i*50:(i+1)*50,1:].squeeze().numpy(),mean[i*50:(i+1)*50], 'k*')\n",
    "            ax[i].fill_between(X_test_tensor[i*50:(i+1)*50,1:].flatten().numpy(), lower[i*50:(i+1)*50].squeeze().numpy(), upper[i*50:(i+1)*50].squeeze().numpy(), alpha=0.5)\n",
    "            ax[i].set_ylim([-3, 3])\n",
    "            ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
