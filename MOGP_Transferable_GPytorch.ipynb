{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gpytorch.kernels import Kernel\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.means import Mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gpytorch.kernels import Kernel, RBFKernel,ScaleKernel\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.settings import cholesky_jitter\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamHandler added\n"
     ]
    }
   ],
   "source": [
    "# 创建一个logger\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建文件处理器，写入日志文件\n",
    "fh = logging.FileHandler('app.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "\n",
    "# 设置日志格式\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# 将处理器添加到logger中\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    print(\"StreamHandler added\")\n",
    "else:\n",
    "    print(\"StreamHandler already exists\")\n",
    "\n",
    "\n",
    "USE_TOY_DATASET=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(X_path,y_path,X_domain_path=None,do_standardisation=False,test_size=0.1,random_state=42):\n",
    "    X_df=None\n",
    "    X_domain_info=None\n",
    "    y_df=None\n",
    "    with open(X_path, 'rb') as f:\n",
    "        X_df = pickle.load(f)\n",
    "    with open(y_path, 'rb') as f:\n",
    "        y_df = pickle.load(f)\n",
    "    print(X_df.shape)\n",
    "    print(y_df.shape)\n",
    "    if X_domain_path!=None:\n",
    "        with open(X_domain_path, 'rb') as f:\n",
    "            X_domain_info = pickle.load(f)\n",
    "        print(X_domain_info.shape)\n",
    "        X_train, X_test, y_train, y_test,X_D_train,X_D_test = train_test_split(X_df, y_df,X_domain_info, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test= train_test_split(X_df, y_df, test_size=test_size, random_state=random_state)\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train = np.array(X_D_train, dtype=np.float32)\n",
    "        X_D_test = np.array(X_D_test, dtype=np.float32)\n",
    "\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "    if do_standardisation:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train_tensor = torch.tensor(X_D_train)\n",
    "        X_D_test_tensor = torch.tensor(X_D_test)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test)\n",
    "    y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "    if X_domain_path!=None: \n",
    "        return (X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)\n",
    "    else:\n",
    "        return (X_train_tensor,y_train_tensor),(X_test_tensor,y_test_tensor)\n",
    "\n",
    "def sigmoid_4_param(x, x0, L, k, d):\n",
    "    \"\"\" Comparing with Dennis Wang's sigmoid:\n",
    "    x0 -  p - position, correlation with IC50 or EC50\n",
    "        bounds [0, 1]\n",
    "    L = 1 in Dennis Wang's sigmoid, protect from devision by zero if x is too small \n",
    "        L<1 inverted sigmoid, l=100 - lower upper and lower boundso sigmpoid on y axis (y= [0.1, 0.11])\n",
    "        bounds [0.8, 10]\n",
    "    k = -1/s (s -shape parameter)  default = -10 k=0 straight line, k<0 sigmoid around k=-10\n",
    "        bounds [1, -100]\n",
    "    d - determines the vertical position of the sigmoid - shift on y axis - better fitting then Dennis Wang's sigmoid\n",
    "         bounds [0, 0.9]\n",
    "    parameters_bound ((0, 0.8, -100, 0), (1, 10, 1, 0.9))\n",
    "    \"\"\"\n",
    "    return ( 1/ (L + np.exp(-k*(x-x0))) + d)\n",
    "\n",
    "def overwrite_to_test():\n",
    "    VAR=0.09\n",
    "    AMP=1.0\n",
    "    train_size=100\n",
    "\n",
    "    X_sin=torch.linspace(0, 1, train_size)\n",
    "    X_cos=torch.linspace(0, 1, train_size)\n",
    "    X_sig=torch.linspace(0, 1, train_size)\n",
    "    y_sin = AMP*torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos = AMP*torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig = AMP*(torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    # Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "    X_train_tensor = torch.linspace(0, 1, train_size)\n",
    "    # True function is sin(2*pi*x) with Gaussian noise\n",
    "\n",
    "    num_conc=1\n",
    "    num_feat=1\n",
    "    nums_domain=torch.Tensor([3])\n",
    "    num_data=train_size\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "    y_sin = y_sin.unsqueeze(1)\n",
    "    y_cos = y_cos.unsqueeze(1)\n",
    "    y_sig = y_sig.unsqueeze(1)\n",
    "\n",
    "    X_sin_domain=torch.zeros(train_size,1)\n",
    "    X_cos_domain=torch.ones(train_size,1)\n",
    "    X_sig_domain=torch.ones(train_size,1)*2\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "    X_train_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_train_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    for i in range(3):\n",
    "        ax[i].plot(X_train_tensor[i*train_size:(i+1)*train_size,1:].squeeze().numpy(),y_train_tensor[i*train_size:(i+1)*train_size], 'k*')\n",
    "        axis=X_train_tensor[i*train_size:(i+1)*train_size,1:].flatten().numpy()\n",
    "        ax[i].set_ylim([-3, 3])\n",
    "        ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "\n",
    "    # indices = torch.randperm(X_train_tensor.size(0))\n",
    "    # X_train_tensor = X_train_tensor[indices]\n",
    "    # y_train_tensor = y_train_tensor[indices]\n",
    "\n",
    "    test_size=50\n",
    "    X_sin=torch.linspace(0, 1, test_size)\n",
    "    X_cos=torch.linspace(0, 1, test_size)\n",
    "    X_sig=torch.linspace(0, 1, test_size)\n",
    "\n",
    "    X_sin_domain=torch.zeros(test_size,1)\n",
    "    X_cos_domain=torch.ones(test_size,1)\n",
    "    X_sig_domain=torch.ones(test_size,1)*2\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "\n",
    "    y_sin =AMP* torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos =AMP* torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig =AMP* (torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "\n",
    "    # X_test_tensor =X_sin_cat\n",
    "    # y_test_tensor =y_sin\n",
    "    \n",
    "    X_test_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_test_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    # X_test_tensor = torch.cat((X_sin_cat, X_cos_cat), dim=0)\n",
    "    # y_test_tensor = torch.cat((y_sin, y_cos), dim=0)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    NUMS_DOMAIN=None\n",
    "    NUMS_DOMAIN_FEATURE=None\n",
    "    NUMS_DOMAIN_AS_INT=None\n",
    "    NUM_CONC=None\n",
    "    STEP_SIZE=None\n",
    "    lr=None\n",
    "    gamma=None\n",
    "    def __init__(self, NUMS_DOMAIN=None,\n",
    "                 NUMS_DOMAIN_FEATURE=None,\n",
    "                 NUMS_DOMAIN_AS_INT=None,\n",
    "                 NUM_CONC=None,\n",
    "                 STEP_SIZE=None,\n",
    "                 lr=None,\n",
    "                 gamma=None):\n",
    "        self.NUMS_DOMAIN=NUMS_DOMAIN\n",
    "        self.NUMS_DOMAIN_FEATURE=NUMS_DOMAIN_FEATURE\n",
    "        self.NUMS_DOMAIN_AS_INT=NUMS_DOMAIN_AS_INT\n",
    "        self.NUM_CONC=NUM_CONC\n",
    "        self.STEP_SIZE=STEP_SIZE\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model,kernel,config):\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=config.NUM_CONC)\n",
    "\n",
    "    m = model(X_train_tensor, y_train_tensor, likelihood,kernel,config)\n",
    "\n",
    "    training_iterations = 500\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    m.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=config.lr)\n",
    "    STEP_SIZE=config.STEP_SIZE\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=config.gamma)\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, m)\n",
    "    last_loss=1\n",
    "    avg_loss=0\n",
    "    this_loss=0\n",
    "    for i in range(training_iterations):\n",
    "        try:\n",
    "            optimizer.zero_grad()\n",
    "            output = m(X_train_tensor)\n",
    "            loss = -mll(output, y_train_tensor)\n",
    "            loss.backward()\n",
    "                \n",
    "            this_loss=loss.item()\n",
    "            avg_loss+=this_loss\n",
    "            optimizer.step()  # 更新参数\n",
    "            scheduler.step()  # 更新学习率\n",
    "            \n",
    "            if i%STEP_SIZE==STEP_SIZE-1:\n",
    "                logger.debug('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "                avg_loss=avg_loss/STEP_SIZE\n",
    "                if abs((this_loss-avg_loss)/avg_loss)<0.001:\n",
    "                    break\n",
    "                        \n",
    "                avg_loss=0\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"发生了一个异常: {e}\")\n",
    "            continue\n",
    "        \n",
    "    m.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        distribution = likelihood(m(X_test_tensor))\n",
    "        mean = distribution.mean\n",
    "        lower, upper = distribution.confidence_region()\n",
    "    nll = -torch.distributions.Normal(mean, distribution.variance.sqrt()).log_prob(y_test_tensor).mean().item()\n",
    "\n",
    "    rmse=torch.sqrt(torch.tensor(mean_squared_error(y_test_tensor.numpy(), mean.numpy()))).item() \n",
    "    nmse = rmse / torch.var(y_test_tensor).item()\n",
    "\n",
    "    logger.info(f'NLL: {nll:.4f}')\n",
    "    logger.info(f'RMSE: {rmse:.4f}')\n",
    "    logger.info(f'NMSE: {nmse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Exponential Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_Exponential_Squared(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.height_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN)))\n",
    "        self.length_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN))) \n",
    "        self.register_parameter(name='height_scales', parameter=self.height_scales)\n",
    "        self.register_parameter(name='length_scales', parameter=self.length_scales)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        # x1_domain   =   x1[:, :num_domain_feat]\n",
    "        # x1_genetics =   x1[:, num_domain_feat:]\n",
    "        # x2_domain   =   x2[:, :num_domain_feat]\n",
    "        # x2_genetics =   x2[:, num_domain_feat:]\n",
    "\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetics =   x1\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_genetics =   x2\n",
    "\n",
    "        dist_mat=self.covar_dist(x1_genetics, x2_genetics, square_dist=True, diag=diag, **params)\n",
    "\n",
    "        height_scales_parsed_1 = self.height_scales[x1_domain.long()].flatten()\n",
    "        height_scales_parsed_2 = self.height_scales[x2_domain.long()].flatten()\n",
    "        length_scales_parsed_1 = self.length_scales[x1_domain.long()].flatten()\n",
    "        length_scales_parsed_2 = self.length_scales[x2_domain.long()].flatten()\n",
    "\n",
    "        part_L1L2T=torch.sqrt(torch.outer(length_scales_parsed_1*length_scales_parsed_1,length_scales_parsed_2.T*length_scales_parsed_2.T))\n",
    "        part_L1L1T=length_scales_parsed_1*length_scales_parsed_1\n",
    "        part_L2L2T=length_scales_parsed_2*length_scales_parsed_2\n",
    "        part_L1sqrL2sqr=torch.outer(part_L1L1T,torch.ones_like(part_L2L2T).T)+torch.outer(torch.ones_like(part_L1L1T),part_L2L2T)\n",
    "\n",
    "        part_1=torch.outer(height_scales_parsed_1,height_scales_parsed_2.T)\n",
    "        part_2=torch.sqrt(2*part_L1L2T/part_L1sqrL2sqr)\n",
    "        part_3=torch.exp(-dist_mat/part_L1sqrL2sqr)\n",
    "\n",
    "        result=part_1*part_2*part_3\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "class K_MS(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.domain_coefficient=torch.nn.Parameter(torch.rand((config.NUMS_DOMAIN))*2-1)\n",
    "        self.K_ES=Kernel_Exponential_Squared(config)\n",
    "        self.register_parameter(name='domain_coefficient', parameter=self.domain_coefficient)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "\n",
    "        domain_coefficient_parsed_1=self.domain_coefficient[x1_domain.long()].flatten()\n",
    "        domain_coefficient_parsed_2=self.domain_coefficient[x2_domain.long()].flatten()\n",
    "        domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
    "\n",
    "        domain_mat=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat == 1.0)\n",
    "\n",
    "        # 等于1的位置置0\n",
    "        domain_mat[mask] = 0.0\n",
    "\n",
    "        # 不等于1的位置置1\n",
    "        domain_mat[~mask] = 1.0\n",
    "        # print(domain_scaler)\n",
    "\n",
    "        \n",
    "        base_cov=self.K_ES(x1,x2,diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
    "\n",
    "        final_scaler=(domain_scaler-1.0)*domain_mat+1.0\n",
    "        return base_cov*final_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_Alpha_Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Alpha_Beta(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernels=[]\n",
    "        for _ in range(config.NUMS_DOMAIN):\n",
    "            self.kernels.append(RBFKernel())\n",
    "\n",
    "        self.alpha= torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "        self.beta=torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):#\n",
    "        x1_domain   =   x1[:, :config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_num_data=x1_domain.shape[0]\n",
    "        x2_num_data=x2_domain.shape[0]\n",
    "        cov_mats_alpha=torch.zeros([x1_num_data,x2_num_data])\n",
    "        cov_mats_beta=torch.zeros([x1_num_data,x2_num_data])\n",
    "\n",
    "        alpha_reparameterized=torch.sqrt(self.alpha*self.alpha)\n",
    "        beta_reparameterized=torch.sqrt(self.beta*self.beta)\n",
    "        for i in range(config.NUMS_DOMAIN):\n",
    "            cov_mats_alpha=cov_mats_alpha.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*alpha_reparameterized[i])\n",
    "            cov_mats_beta=cov_mats_beta.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*beta_reparameterized[i])\n",
    "\n",
    "        domain_mat_alpha=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        domain_mat_beta=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat_alpha == 1.0)\n",
    "        domain_mat_alpha[mask] = 0.0\n",
    "        domain_mat_alpha[~mask] = 1.0\n",
    "        domain_mat_beta[mask] = 1.0\n",
    "        domain_mat_beta[~mask] = 0.0\n",
    "\n",
    "        \n",
    "        result=cov_mats_alpha*domain_mat_alpha+cov_mats_beta*domain_mat_beta\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferable quadriple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transferable_Quadriple_Kernel(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # self.kernel_mu = gpytorch.kernels.RBFKernel()\n",
    "        # self.kernel_met = gpytorch.kernels.RBFKernel()\n",
    "        # self.kernel_cn = gpytorch.kernels.RBFKernel()\n",
    "        # self.kernel_dc = gpytorch.kernels.RBFKernel()\n",
    "        # self.filter_mu,self.filter_met,self.filter_cn,self.filter_dc=filters\n",
    "        self.temp=RBFKernel()\n",
    "        initial_value = torch.ones(config.NUMS_DOMAIN)\n",
    "        self.domain_relateness = torch.nn.Parameter(initial_value)\n",
    "\n",
    "        # 使用 softplus 变换来确保参数为正值\n",
    "        self.register_constraint(\"domain_relateness\", gpytorch.constraints.Positive())\n",
    "        \n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetics =   x1[:, self.config.NUMS_DOMAIN_FEATURE:]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_genetics =   x2[:, self.config.NUMS_DOMAIN_FEATURE:]\n",
    "        # # Apply the base kernel\n",
    "        # filter_mu=self.filter_mu\n",
    "        # filter_met=self.filter_met\n",
    "        # filter_cn=self.filter_cn\n",
    "        # filter_dc=self.filter_dc\n",
    "        \n",
    "        # x1_mu   =   x1_genetics @ filter_mu\n",
    "        # x1_met  =   x1_genetics @ filter_met\n",
    "        # x1_cn   =   x1_genetics @ filter_cn\n",
    "        # x1_dc   =   x1_genetics @ filter_dc\n",
    "\n",
    "        # x2_mu   =   x2_genetics @ filter_mu\n",
    "        # x2_met  =   x2_genetics @ filter_met\n",
    "        # x2_cn   =   x2_genetics @ filter_cn\n",
    "        # x2_dc   =   x2_genetics @ filter_dc\n",
    "\n",
    "        domain_relateness_multiplier=self.domain_relateness\n",
    "        # 获取领域系数\n",
    "        domain_factors1 = self.domain_relateness[x1_domain.long()]\n",
    "        domain_factors2 = self.domain_relateness[x2_domain.long()]\n",
    "\n",
    "        # base_cov=self.kernel_mu(x1_mu,x2_mu)*self.kernel_met(x1_met,x2_met)*self.kernel_cn(x1_cn,x2_cn)* 1#self.kernel_dc(x1_dc,x2_dc)\n",
    "        base_cov=self.temp(x1_genetics,x2_genetics)\n",
    "        # print(f\"Base kernel:{base_cov.shape}\")\n",
    "        if diag:\n",
    "            adjusted_cov = base_cov * (domain_factors1 * domain_factors2).squeeze()\n",
    "            # print(f\"Diag Adjusted kernel:{adjusted_cov.shape}\")\n",
    "        else:\n",
    "            factor= (domain_factors1 @ domain_factors2.T)\n",
    "            adjusted_cov = base_cov *factor\n",
    "            # print(f\"Non-diag Adjusted kernel:{adjusted_cov.shape}\")\n",
    "            # print(f\"factor:{factor.shape}\")\n",
    "            # print(f\"domain_factors1:{domain_factors1.shape}\")\n",
    "            # print(f\"domain_factors2:{domain_factors2.shape}\")\n",
    "        return adjusted_cov\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model of corregionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Model_Of_Corregionalization(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(Linear_Model_Of_Corregionalization, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        if kernel==RBFKernel:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel())\n",
    "        else:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel(config))\n",
    "        self.covar_module =gpytorch.kernels.LCMKernel(\n",
    "            kernels, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "\n",
    "        logger.debug(\"Completed\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGP(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(MultitaskGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        \n",
    "        if kernel==RBFKernel:\n",
    "            kern=kernel()\n",
    "        else:\n",
    "            kern=kernel(config)\n",
    "                \n",
    "        self.covar_module =gpytorch.kernels.MultitaskKernel(\n",
    "            kern, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "\n",
    "        \n",
    "        if USE_TOY_DATASET:\n",
    "            self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "                [RBFKernel(),  # 2 latent GP's\n",
    "             RBFKernel(),RBFKernel()], num_tasks=config.NUM_CONC,rank=config.NUM_CONC\n",
    "            )\n",
    "        #     self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "        #         K_MS(),  # 2 latent GP's\n",
    "\n",
    "        #     num_tasks=num_conc, rank=num_conc-1\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 1084)\n",
      "(62, 10)\n",
      "(62, 1)\n",
      "torch.Size([55, 1084])\n",
      "torch.Size([55, 1])\n",
      "torch.Size([55, 10])\n",
      "13.0\n",
      "torch.Size([55, 1085])\n",
      "torch.Size([7, 1085])\n",
      "MUT features:310.0torch.Size([1084, 1084])\n",
      "Met features:338.0torch.Size([1084, 1084])\n",
      "CNA features:425.0torch.Size([1084, 1084])\n",
      "DC features:0.0torch.Size([1084, 1084])\n"
     ]
    }
   ],
   "source": [
    "# X_path=\"data/X_df_toy.pkl\"\n",
    "# y_path=\"data/y_df_toy.pkl\"\n",
    "\n",
    "# X_path=\"data/X_df_Shikonin.pkl\"\n",
    "# y_path=\"data/y_df_Shikonin.pkl\"\n",
    "# X_domain_path=\"data/X_domain_info_Shikonin.pkl\"\n",
    "\n",
    "Global=config()\n",
    "\n",
    "X_path=\"data/X_df_2_shots.pkl\"\n",
    "y_path=\"data/y_df_2_shots.pkl\"\n",
    "X_domain_path=\"data/X_domain_info_2_shots.pkl\"\n",
    "\n",
    "# X_path=\"data/2_shots_4_params_sigmoid/X_df.pkl\"\n",
    "# y_path=\"data/2_shots_4_params_sigmoid/y_df.pkl\"\n",
    "# X_domain_path=\"data/2_shots_4_params_sigmoid/X_domain_info.pkl\"\n",
    "(X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)=load_dataset(X_path,y_path,do_standardisation=True,X_domain_path=X_domain_path)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(X_D_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "Global.NUM_CONC=y_train_tensor.shape[1]\n",
    "\n",
    "Global.NUM_FEAT=X_train_tensor.shape[1]\n",
    "Global.NUM_DOMAIN_FEAT=X_D_train_tensor.shape[1]\n",
    "NUMS_DOMAIN, max_indices_row = torch.max(X_D_train_tensor, dim=0)\n",
    "NUMS_DOMAIN.add_(1)\n",
    "\n",
    "\n",
    "print(NUMS_DOMAIN.item())\n",
    "Global.NUMS_DOMAIN=NUMS_DOMAIN.long()\n",
    "X_train_tensor = torch.cat((X_D_train_tensor, X_train_tensor), dim=1)\n",
    "X_test_tensor = torch.cat((X_D_test_tensor, X_test_tensor), dim=1)\n",
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n",
    "\n",
    "with open(X_path, 'rb') as f:\n",
    "    X_df = pickle.load(f)\n",
    "mask_bool_met = X_df.columns.str.contains(\"HypMET\")\n",
    "mask_bool_mut = X_df.columns.str.contains(\"mut\")\n",
    "mask_bool_cna = X_df.columns.str.contains(\"cna\")\n",
    "mask_bool_dc  = X_df.columns.str.contains(\"dc\")\n",
    "\n",
    "mask_float_met = np.array(mask_bool_met, dtype=np.float32) \n",
    "mask_float_mut = np.array(mask_bool_mut, dtype=np.float32)\n",
    "mask_float_cna = np.array(mask_bool_cna, dtype=np.float32)\n",
    "mask_float_dc  = np.array(mask_bool_dc, dtype=np.float32)\n",
    "\n",
    "diag_matrix_met = torch.tensor(np.diag(mask_float_met))\n",
    "diag_matrix_mut = torch.tensor(np.diag(mask_float_mut))\n",
    "diag_matrix_cna = torch.tensor(np.diag(mask_float_cna))\n",
    "diag_matrix_dc  = torch.tensor(np.diag(mask_float_dc))\n",
    "filters=(diag_matrix_mut,diag_matrix_met,diag_matrix_cna,diag_matrix_dc)\n",
    "print(\"MUT features:\"+str(np.sum(mask_float_mut))+str(diag_matrix_met.shape))\n",
    "print(\"Met features:\"+str(np.sum(mask_float_met))+str(diag_matrix_met.shape))\n",
    "print(\"CNA features:\"+str(np.sum(mask_float_cna))+str(diag_matrix_met.shape))\n",
    "print(\"DC features:\"+str(np.sum(mask_float_dc))+str(diag_matrix_met.shape))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "Global.lr=0.1\n",
    "Global.gamma=0.5\n",
    "Global.STEP_SIZE=50\n",
    "Global.NUMS_DOMAIN_FEATURE=1\n",
    "print(Global.NUMS_DOMAIN[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 19:35:42,958 - DEBUG - Completed\n",
      "C:\\Users\\25858\\AppData\\Local\\Temp\\ipykernel_11444\\3416435673.py:14: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\b\\abs_8f7uhuge1i\\croot\\pytorch-select_1717607507421\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3679.)\n",
      "  domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
      "2024-08-12 19:35:46,643 - DEBUG - Iter 50/500 - Loss: -0.646\n",
      "2024-08-12 19:35:49,329 - DEBUG - Iter 100/500 - Loss: -1.095\n",
      "2024-08-12 19:35:51,974 - DEBUG - Iter 150/500 - Loss: -1.210\n",
      "2024-08-12 19:35:54,754 - DEBUG - Iter 200/500 - Loss: -1.244\n",
      "2024-08-12 19:35:57,527 - DEBUG - Iter 250/500 - Loss: -1.257\n",
      "2024-08-12 19:36:00,151 - DEBUG - Iter 300/500 - Loss: -1.263\n",
      "2024-08-12 19:36:02,624 - DEBUG - Iter 350/500 - Loss: -1.266\n",
      "2024-08-12 19:36:05,305 - DEBUG - Iter 400/500 - Loss: -1.268\n",
      "2024-08-12 19:36:05,363 - INFO - NLL: -1.1839\n",
      "2024-08-12 19:36:05,363 - INFO - RMSE: 0.1676\n",
      "2024-08-12 19:36:05,364 - INFO - NMSE: 0.8594\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models=[MultitaskGP,Linear_Model_Of_Corregionalization]\n",
    "kernels=[K_MS,K_Alpha_Beta,RBFKernel]\n",
    "\n",
    "run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model=Linear_Model_Of_Corregionalization,kernel=K_MS,config=Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TOY_DATASET == True:\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(X_test_tensor))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize plot\n",
    "        f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        mean=observed_pred.mean.numpy()\n",
    "        for i in range(3):\n",
    "            ax[i].plot(X_test_tensor[i*50:(i+1)*50,1:].squeeze().numpy(),mean[i*50:(i+1)*50], 'k*')\n",
    "            ax[i].fill_between(X_test_tensor[i*50:(i+1)*50,1:].flatten().numpy(), lower[i*50:(i+1)*50].squeeze().numpy(), upper[i*50:(i+1)*50].squeeze().numpy(), alpha=0.5)\n",
    "            ax[i].set_ylim([-3, 3])\n",
    "            ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
