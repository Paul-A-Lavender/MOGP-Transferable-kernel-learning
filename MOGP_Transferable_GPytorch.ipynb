{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gpytorch.kernels import Kernel\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.means import Mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gpytorch.kernels import Kernel, RBFKernel,ScaleKernel\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.settings import cholesky_jitter\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from itertools import product\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Subset,TensorDataset\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamHandler added\n",
      "GPU is not available!\n"
     ]
    }
   ],
   "source": [
    "# 创建一个logger\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建文件处理器，写入日志文件\n",
    "fh = logging.FileHandler('app.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "\n",
    "# 设置日志格式\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# 将处理器添加到logger中\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    print(\"StreamHandler added\")\n",
    "else:\n",
    "    print(\"StreamHandler already exists\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available!\")\n",
    "USE_TOY_DATASET=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(X_path,y_path,X_domain_path=None,do_standardisation=False,test_size=0.1,random_state=42):\n",
    "    X_df=None\n",
    "    X_domain_info=None\n",
    "    y_df=None\n",
    "    with open(X_path, 'rb') as f:\n",
    "        X_df = pickle.load(f)\n",
    "    with open(y_path, 'rb') as f:\n",
    "        y_df = pickle.load(f)\n",
    "    print(X_df.shape)\n",
    "    print(y_df.shape)\n",
    "    if X_domain_path!=None:\n",
    "        with open(X_domain_path, 'rb') as f:\n",
    "            X_domain_info = pickle.load(f)\n",
    "        print(X_domain_info.shape)\n",
    "        X_train, X_test, y_train, y_test,X_D_train,X_D_test = train_test_split(X_df, y_df,X_domain_info, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test= train_test_split(X_df, y_df, test_size=test_size, random_state=random_state)\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train = np.array(X_D_train, dtype=np.float32)\n",
    "        X_D_test = np.array(X_D_test, dtype=np.float32)\n",
    "\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "    if do_standardisation:\n",
    "        print(\"Performing standardisation\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train_tensor = torch.tensor(X_D_train)\n",
    "        X_D_test_tensor = torch.tensor(X_D_test)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test)\n",
    "    y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "    if X_domain_path!=None: \n",
    "        return (X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)\n",
    "    else:\n",
    "        return (X_train_tensor,y_train_tensor),(X_test_tensor,y_test_tensor)\n",
    "\n",
    "def sigmoid_4_param(x, x0, L, k, d):\n",
    "    \"\"\" Comparing with Dennis Wang's sigmoid:\n",
    "    x0 -  p - position, correlation with IC50 or EC50\n",
    "        bounds [0, 1]\n",
    "    L = 1 in Dennis Wang's sigmoid, protect from devision by zero if x is too small \n",
    "        L<1 inverted sigmoid, l=100 - lower upper and lower boundso sigmpoid on y axis (y= [0.1, 0.11])\n",
    "        bounds [0.8, 10]\n",
    "    k = -1/s (s -shape parameter)  default = -10 k=0 straight line, k<0 sigmoid around k=-10\n",
    "        bounds [1, -100]\n",
    "    d - determines the vertical position of the sigmoid - shift on y axis - better fitting then Dennis Wang's sigmoid\n",
    "         bounds [0, 0.9]\n",
    "    parameters_bound ((0, 0.8, -100, 0), (1, 10, 1, 0.9))\n",
    "    \"\"\"\n",
    "    return ( 1/ (L + np.exp(-k*(x-x0))) + d)\n",
    "\n",
    "def overwrite_to_test():\n",
    "    VAR=0.09\n",
    "    AMP=1.0\n",
    "    train_size=100\n",
    "\n",
    "    X_sin=torch.linspace(0, 1, train_size)\n",
    "    X_cos=torch.linspace(0, 1, train_size)\n",
    "    X_sig=torch.linspace(0, 1, train_size)\n",
    "    y_sin = AMP*torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos = AMP*torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig = AMP*(torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    # Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "    X_train_tensor = torch.linspace(0, 1, train_size)\n",
    "    # True function is sin(2*pi*x) with Gaussian noise\n",
    "\n",
    "    num_conc=1\n",
    "    num_feat=1\n",
    "    nums_domain=torch.Tensor([3])\n",
    "    num_data=train_size\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "    y_sin = y_sin.unsqueeze(1)\n",
    "    y_cos = y_cos.unsqueeze(1)\n",
    "    y_sig = y_sig.unsqueeze(1)\n",
    "\n",
    "    X_sin_domain=torch.zeros(train_size,1)\n",
    "    X_cos_domain=torch.ones(train_size,1)\n",
    "    X_sig_domain=torch.ones(train_size,1)*2\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "    X_train_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_train_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    for i in range(3):\n",
    "        ax[i].plot(X_train_tensor[i*train_size:(i+1)*train_size,1:].squeeze().numpy(),y_train_tensor[i*train_size:(i+1)*train_size], 'k*')\n",
    "        axis=X_train_tensor[i*train_size:(i+1)*train_size,1:].flatten().numpy()\n",
    "        ax[i].set_ylim([-3, 3])\n",
    "        ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "\n",
    "    # indices = torch.randperm(X_train_tensor.size(0))\n",
    "    # X_train_tensor = X_train_tensor[indices]\n",
    "    # y_train_tensor = y_train_tensor[indices]\n",
    "\n",
    "    test_size=50\n",
    "    X_sin=torch.linspace(0, 1, test_size)\n",
    "    X_cos=torch.linspace(0, 1, test_size)\n",
    "    X_sig=torch.linspace(0, 1, test_size)\n",
    "\n",
    "    X_sin_domain=torch.zeros(test_size,1)\n",
    "    X_cos_domain=torch.ones(test_size,1)\n",
    "    X_sig_domain=torch.ones(test_size,1)*2\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "\n",
    "    y_sin =AMP* torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos =AMP* torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig =AMP* (torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "\n",
    "    # X_test_tensor =X_sin_cat\n",
    "    # y_test_tensor =y_sin\n",
    "    \n",
    "    X_test_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_test_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    # X_test_tensor = torch.cat((X_sin_cat, X_cos_cat), dim=0)\n",
    "    # y_test_tensor = torch.cat((y_sin, y_cos), dim=0)  \n",
    "\n",
    "def dataloader2tensor(data_loader):\n",
    "    all_inputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 遍历 DataLoader 并将所有批次数据合并成一个大张量\n",
    "    for inputs, labels in data_loader:\n",
    "        all_inputs.append(inputs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # 使用 torch.cat 将所有小批量张量合并成一个大张量\n",
    "    X_tensor = torch.cat(all_inputs, dim=0)\n",
    "    y_tensor = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # 现在，X_train_tensor 和 y_train_tensor 是重新合并后的大张量\n",
    "    print(f\"X_train_tensor shape: {X_tensor.shape}\")\n",
    "    print(f\"y_train_tensor shape: {y_tensor.shape}\")\n",
    "    return X_tensor,y_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    NUMS_DOMAIN=None\n",
    "    NUMS_DOMAIN_FEATURE=None\n",
    "    NUMS_DOMAIN_AS_INT=None\n",
    "    NUM_CONC=None\n",
    "    STEP_SIZE=None\n",
    "    lr=None\n",
    "    gamma=None\n",
    "    def __init__(self, NUMS_DOMAIN=None,\n",
    "                 NUMS_DOMAIN_FEATURE=None,\n",
    "                 NUMS_DOMAIN_AS_INT=None,\n",
    "                 NUM_CONC=None,\n",
    "                 STEP_SIZE=None,\n",
    "                 lr=None,\n",
    "                 gamma=None,\n",
    "                 NUM_FEAT=None,):\n",
    "        self.NUMS_DOMAIN=NUMS_DOMAIN\n",
    "        self.NUMS_DOMAIN_FEATURE=NUMS_DOMAIN_FEATURE\n",
    "        self.NUMS_DOMAIN_AS_INT=NUMS_DOMAIN_AS_INT\n",
    "        self.NUM_CONC=NUM_CONC\n",
    "        self.STEP_SIZE=STEP_SIZE\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n",
    "        self.NUM_FEAT=NUM_FEAT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model,kernel,config):\n",
    "    logger.info(f'training starts for model {str(model)} with kernel {str(kernel)}; lr: {config.lr}; step_size:{config.STEP_SIZE}; gamma:{config.gamma}')\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=config.NUM_CONC)\n",
    "\n",
    "    m = model(X_train_tensor, y_train_tensor, likelihood,kernel,config)\n",
    "\n",
    "    training_iterations = 500\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    m.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=config.lr)\n",
    "    STEP_SIZE=config.STEP_SIZE\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=config.gamma)\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, m)\n",
    "    last_loss=1\n",
    "    avg_loss=0\n",
    "    this_loss=0\n",
    "    for i in range(training_iterations):\n",
    "        # try:\n",
    "            optimizer.zero_grad()\n",
    "            output = m(X_train_tensor)\n",
    "            loss = -mll(output, y_train_tensor)\n",
    "            loss.backward()\n",
    "                \n",
    "            this_loss=loss.item()\n",
    "            avg_loss+=this_loss\n",
    "            optimizer.step()  # 更新参数\n",
    "            scheduler.step()  # 更新学习率\n",
    "            \n",
    "            if i%STEP_SIZE==STEP_SIZE-1:\n",
    "                logger.debug('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "                avg_loss=avg_loss/STEP_SIZE\n",
    "                if abs((this_loss-avg_loss)/avg_loss)<0.01:\n",
    "                    logger.info(f'Early cut off at epoch {i} with loss of {this_loss }')\n",
    "                    break\n",
    "                        \n",
    "                avg_loss=0\n",
    "                    \n",
    "        # except Exception as e:\n",
    "        #     logger.error(f\"发生了一个异常: {e}\")\n",
    "        #     continue\n",
    "        \n",
    "    m.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        distribution = likelihood(m(X_test_tensor))\n",
    "        mean = distribution.mean\n",
    "        lower, upper = distribution.confidence_region()\n",
    "    nll = -torch.distributions.Normal(mean, distribution.variance.sqrt()).log_prob(y_test_tensor).mean().item()\n",
    "\n",
    "    rmse=torch.sqrt(torch.tensor(mean_squared_error(y_test_tensor.numpy(), mean.numpy()))).item() \n",
    "    nmse = rmse / torch.var(y_test_tensor).item()\n",
    "\n",
    "    logger.info(f'NLL: {nll:.4f}; RMSE: {rmse:.4f}; NMSE: {nmse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Exponential Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_Exponential_Squared(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.height_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN)))\n",
    "        self.length_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN))) \n",
    "        self.register_parameter(name='height_scales', parameter=self.height_scales)\n",
    "        self.register_parameter(name='length_scales', parameter=self.length_scales)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        # x1_domain   =   x1[:, :num_domain_feat]\n",
    "        # x1_genetics =   x1[:, num_domain_feat:]\n",
    "        # x2_domain   =   x2[:, :num_domain_feat]\n",
    "        # x2_genetics =   x2[:, num_domain_feat:]\n",
    "\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetics =   x1\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_genetics =   x2\n",
    "\n",
    "        dist_mat=self.covar_dist(x1_genetics, x2_genetics, square_dist=True, diag=diag, **params)\n",
    "        height_scales_parsed_1 = self.height_scales[x1_domain.long()].flatten()\n",
    "        height_scales_parsed_2 = self.height_scales[x2_domain.long()].flatten()\n",
    "        length_scales_parsed_1 = self.length_scales[x1_domain.long()].flatten()\n",
    "        length_scales_parsed_2 = self.length_scales[x2_domain.long()].flatten()\n",
    "\n",
    "        part_L1L2T=torch.sqrt(torch.outer(length_scales_parsed_1*length_scales_parsed_1,length_scales_parsed_2.T*length_scales_parsed_2.T))\n",
    "        part_L1L1T=length_scales_parsed_1*length_scales_parsed_1\n",
    "        part_L2L2T=length_scales_parsed_2*length_scales_parsed_2\n",
    "        part_L1sqrL2sqr=torch.outer(part_L1L1T,torch.ones_like(part_L2L2T).T)+torch.outer(torch.ones_like(part_L1L1T),part_L2L2T)\n",
    "\n",
    "        part_1=torch.outer(height_scales_parsed_1,height_scales_parsed_2.T)\n",
    "        part_2=torch.sqrt(2*part_L1L2T/part_L1sqrL2sqr)\n",
    "        part_3=torch.exp(-dist_mat/part_L1sqrL2sqr)\n",
    "\n",
    "        result=part_1*part_2*part_3\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class K_MS(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.domain_coefficient=torch.nn.Parameter(torch.rand((config.NUMS_DOMAIN))*2-1)\n",
    "        self.K_ES=Kernel_Exponential_Squared(config)\n",
    "        self.register_parameter(name='domain_coefficient', parameter=self.domain_coefficient)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "\n",
    "        domain_coefficient_parsed_1=self.domain_coefficient[x1_domain.long()].flatten()\n",
    "        domain_coefficient_parsed_2=self.domain_coefficient[x2_domain.long()].flatten()\n",
    "        domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
    "\n",
    "        domain_mat=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat == 1.0)\n",
    "\n",
    "        # 等于1的位置置0\n",
    "        domain_mat[mask] = 0.0\n",
    "\n",
    "        # 不等于1的位置置1\n",
    "        domain_mat[~mask] = 1.0\n",
    "        # print(domain_scaler)\n",
    "\n",
    "        \n",
    "        base_cov=self.K_ES(x1,x2,diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
    "\n",
    "        final_scaler=(domain_scaler-1.0)*domain_mat+1.0\n",
    "        return base_cov*final_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_Alpha_Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Alpha_Beta(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernels=[]\n",
    "        for _ in range(config.NUMS_DOMAIN):\n",
    "            self.kernels.append(RBFKernel())\n",
    "        self.config=config\n",
    "        self.alpha= torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "        self.beta=torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):#\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_num_data=x1_domain.shape[0]\n",
    "        x2_num_data=x2_domain.shape[0]\n",
    "        cov_mats_alpha=torch.zeros([x1_num_data,x2_num_data])\n",
    "        cov_mats_beta=torch.zeros([x1_num_data,x2_num_data])\n",
    "\n",
    "        alpha_reparameterized=torch.sqrt(self.alpha*self.alpha)\n",
    "        beta_reparameterized=torch.sqrt(self.beta*self.beta)\n",
    "        for i in range(self.config.NUMS_DOMAIN):\n",
    "            cov_mats_alpha=cov_mats_alpha.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*alpha_reparameterized[i])\n",
    "            cov_mats_beta=cov_mats_beta.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*beta_reparameterized[i])\n",
    "\n",
    "        domain_mat_alpha=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        domain_mat_beta=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat_alpha == 1.0)\n",
    "        domain_mat_alpha[mask] = 0.0\n",
    "        domain_mat_alpha[~mask] = 1.0\n",
    "        domain_mat_beta[mask] = 1.0\n",
    "        domain_mat_beta[~mask] = 0.0\n",
    "\n",
    "        \n",
    "        result=cov_mats_alpha*domain_mat_alpha+cov_mats_beta*domain_mat_beta\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferable quadriple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class K_MS_with_Feat_Scaling(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.domain_coefficient=torch.nn.Parameter(torch.rand(config.NUMS_DOMAIN)*2-1)\n",
    "        self.K_ES=Kernel_Exponential_Squared(config)\n",
    "        self.register_parameter(name='domain_coefficient', parameter=self.domain_coefficient)\n",
    "        \n",
    "        \n",
    "        feature_relateness_init=torch.ones(config.NUM_FEAT)\n",
    "        self.feature_relateness=torch.nn.Parameter(feature_relateness_init)\n",
    "        self.register_parameter(name='feature_relateness', parameter=self.feature_relateness)\n",
    "        self.config=config\n",
    "        \n",
    "    \n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetic   =   x1[:, self.config.NUMS_DOMAIN_FEATURE:]\n",
    "        x2_genetic   =   x2[:, self.config.NUMS_DOMAIN_FEATURE:]        \n",
    "        \n",
    "        feature_scaler_rep=(torch.tanh(self.feature_relateness)+1.0)\n",
    "        x1_scaled=x1_genetic*torch.outer(torch.ones(x1.shape[0]),feature_scaler_rep)\n",
    "        x2_scaled=x2_genetic*torch.outer(torch.ones(x2.shape[0]),feature_scaler_rep)\n",
    "        domain_coefficient_parsed_1=self.domain_coefficient[x1_domain.long()].flatten()\n",
    "        domain_coefficient_parsed_2=self.domain_coefficient[x2_domain.long()].flatten()\n",
    "        domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
    "        domain_mat=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat == 1.0)\n",
    "\n",
    "        # 等于1的位置置0\n",
    "        domain_mat[mask] = 0.0\n",
    "\n",
    "        # 不等于1的位置置1\n",
    "        domain_mat[~mask] = 1.0\n",
    "        # print(domain_scaler)\n",
    "        x1_cat=torch.cat((x1_domain, x1_scaled), dim=1)\n",
    "        x2_cat=torch.cat((x2_domain, x2_scaled), dim=1)\n",
    "        base_cov=self.K_ES(x1_cat,x2_cat,diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
    "\n",
    "        final_scaler=(domain_scaler-1.0)*domain_mat+1.0\n",
    "        return base_cov*final_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model of corregionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Model_Of_Corregionalization(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(Linear_Model_Of_Corregionalization, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        if kernel==RBFKernel:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel())\n",
    "        else:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel(config))\n",
    "        self.covar_module =gpytorch.kernels.LCMKernel(\n",
    "            kernels, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "\n",
    "        logger.debug(\"Completed\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGP(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(MultitaskGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        \n",
    "        if kernel==RBFKernel:\n",
    "            kern=kernel()\n",
    "        else:\n",
    "            kern=kernel(config)\n",
    "                \n",
    "        self.covar_module =gpytorch.kernels.MultitaskKernel(\n",
    "            kern, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "        #     self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "        #         K_MS(),  # 2 latent GP's\n",
    "\n",
    "        #     num_tasks=num_conc, rank=num_conc-1\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 1084)\n",
      "(62, 10)\n",
      "(62, 1)\n",
      "torch.Size([55, 1084])\n",
      "torch.Size([55, 1])\n",
      "torch.Size([55, 10])\n",
      "NUMS_DOMAIN: 2.0\n",
      "torch.Size([55, 1085])\n",
      "torch.Size([7, 1085])\n"
     ]
    }
   ],
   "source": [
    "# X_path=\"data/X_df_toy.pkl\"\n",
    "# y_path=\"data/y_df_toy.pkl\"\n",
    "\n",
    "# X_path=\"data/X_df_Shikonin.pkl\"\n",
    "# y_path=\"data/y_df_Shikonin.pkl\"\n",
    "# X_domain_path=\"data/X_domain_info_Shikonin.pkl\"\n",
    "\n",
    "Global=config()\n",
    "\n",
    "# X_path=\"data/X_df_2_shots.pkl\"\n",
    "# y_path=\"data/y_df_2_shots.pkl\"\n",
    "# X_domain_path=\"data/X_domain_info_2_shots.pkl\"\n",
    "\n",
    "# X_path=\"data/2_shots_4_params_sigmoid/X_df.pkl\"\n",
    "# y_path=\"data/2_shots_4_params_sigmoid/y_df.pkl\"\n",
    "# X_domain_path=\"data/2_shots_4_params_sigmoid/X_domain_info.pkl\"\n",
    "\n",
    "X_path=\"data/2_shots/X_df.pkl\"\n",
    "y_path=\"data/2_shots/y_df.pkl\"\n",
    "X_domain_path=\"data/2_shots/X_domain_info.pkl\"\n",
    "(X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)=load_dataset(X_path,y_path,X_domain_path=X_domain_path)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(X_D_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "Global.NUM_CONC=y_train_tensor.shape[1]\n",
    "\n",
    "Global.NUM_FEAT=X_train_tensor.shape[1]\n",
    "Global.NUM_DOMAIN_FEAT=X_D_train_tensor.shape[1]\n",
    "NUMS_DOMAIN, max_indices_row = torch.max(X_D_train_tensor, dim=0)\n",
    "NUMS_DOMAIN.add_(1)\n",
    "\n",
    "\n",
    "print(f'NUMS_DOMAIN: {NUMS_DOMAIN.item()}')\n",
    "Global.NUMS_DOMAIN=NUMS_DOMAIN.long()\n",
    "X_train_tensor = torch.cat((X_D_train_tensor, X_train_tensor), dim=1)\n",
    "X_test_tensor = torch.cat((X_D_test_tensor, X_test_tensor), dim=1)\n",
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n",
    "\n",
    "with open(X_path, 'rb') as f:\n",
    "    X_df = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Global.lr=0.1\n",
    "Global.gamma=0.5\n",
    "Global.STEP_SIZE=50\n",
    "Global.NUMS_DOMAIN_FEATURE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params= [\n",
    "    [MultitaskGP, RBFKernel, 0.18, 55, 0.8],\n",
    "    [Linear_Model_Of_Corregionalization, RBFKernel, 0.69, 32, 0.5],\n",
    "    [MultitaskGP, K_MS, 0.52, 100, 0.5],\n",
    "    [Linear_Model_Of_Corregionalization, K_MS, 0.18, 100, 0.8],\n",
    "    [MultitaskGP, K_Alpha_Beta, 0.18, 32, 0.2],\n",
    "    [Linear_Model_Of_Corregionalization, K_Alpha_Beta, 0.69, 77, 0.2],\n",
    "    [MultitaskGP, K_MS_with_Feat_Scaling, 0.18, 77.5, 0.35],\n",
    "    [Linear_Model_Of_Corregionalization, K_MS_with_Feat_Scaling, 0.18, 33, 0.35]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 00:29:40,573 - INFO - FOLD 1\n",
      "2024-09-02 00:29:40,575 - INFO - training starts for model <class '__main__.MultitaskGP'> with kernel <class 'gpytorch.kernels.rbf_kernel.RBFKernel'>; lr: 0.18; step_size:55; gamma:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "X_train_tensor shape: torch.Size([55, 1085])\n",
      "y_train_tensor shape: torch.Size([55, 10])\n",
      "X_train_tensor shape: torch.Size([7, 1085])\n",
      "y_train_tensor shape: torch.Size([7, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 00:29:41,178 - DEBUG - Iter 55/500 - Loss: -0.511\n",
      "2024-09-02 00:29:41,784 - DEBUG - Iter 110/500 - Loss: -0.336\n",
      "2024-09-02 00:29:42,357 - DEBUG - Iter 165/500 - Loss: -0.800\n",
      "2024-09-02 00:29:42,931 - DEBUG - Iter 220/500 - Loss: -1.141\n",
      "2024-09-02 00:29:43,500 - DEBUG - Iter 275/500 - Loss: -1.189\n",
      "2024-09-02 00:29:44,079 - DEBUG - Iter 330/500 - Loss: -1.198\n",
      "2024-09-02 00:29:44,079 - INFO - Early cut off at epoch 329 with loss of -1.1977993249893188\n",
      "2024-09-02 00:29:44,091 - INFO - NLL: -1.1500; RMSE: 0.1854; NMSE: 0.9440\n",
      "2024-09-02 00:29:44,092 - INFO - FOLD 2\n",
      "2024-09-02 00:29:44,094 - INFO - training starts for model <class '__main__.MultitaskGP'> with kernel <class 'gpytorch.kernels.rbf_kernel.RBFKernel'>; lr: 0.18; step_size:55; gamma:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "X_train_tensor shape: torch.Size([55, 1085])\n",
      "y_train_tensor shape: torch.Size([55, 10])\n",
      "X_train_tensor shape: torch.Size([7, 1085])\n",
      "y_train_tensor shape: torch.Size([7, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 00:29:44,680 - DEBUG - Iter 55/500 - Loss: -0.593\n",
      "2024-09-02 00:29:45,253 - DEBUG - Iter 110/500 - Loss: -0.444\n",
      "2024-09-02 00:29:45,847 - DEBUG - Iter 165/500 - Loss: -0.736\n",
      "2024-09-02 00:29:46,423 - DEBUG - Iter 220/500 - Loss: -0.856\n",
      "2024-09-02 00:29:46,994 - DEBUG - Iter 275/500 - Loss: -0.872\n",
      "2024-09-02 00:29:46,995 - INFO - Early cut off at epoch 274 with loss of -0.8722142577171326\n",
      "2024-09-02 00:29:47,005 - INFO - NLL: 0.2274; RMSE: 0.2467; NMSE: 1.2148\n",
      "2024-09-02 00:29:47,006 - INFO - FOLD 3\n",
      "2024-09-02 00:29:47,008 - INFO - training starts for model <class '__main__.MultitaskGP'> with kernel <class 'gpytorch.kernels.rbf_kernel.RBFKernel'>; lr: 0.18; step_size:55; gamma:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "X_train_tensor shape: torch.Size([56, 1085])\n",
      "y_train_tensor shape: torch.Size([56, 10])\n",
      "X_train_tensor shape: torch.Size([6, 1085])\n",
      "y_train_tensor shape: torch.Size([6, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 00:29:47,616 - DEBUG - Iter 55/500 - Loss: -0.418\n",
      "2024-09-02 00:29:48,200 - DEBUG - Iter 110/500 - Loss: -0.379\n",
      "2024-09-02 00:29:48,792 - DEBUG - Iter 165/500 - Loss: -0.582\n",
      "2024-09-02 00:29:49,364 - DEBUG - Iter 220/500 - Loss: -0.743\n",
      "2024-09-02 00:29:49,950 - DEBUG - Iter 275/500 - Loss: -0.691\n",
      "2024-09-02 00:29:50,517 - DEBUG - Iter 330/500 - Loss: -0.829\n",
      "2024-09-02 00:29:51,089 - DEBUG - Iter 385/500 - Loss: -0.998\n",
      "2024-09-02 00:29:51,680 - DEBUG - Iter 440/500 - Loss: -0.996\n",
      "2024-09-02 00:29:52,268 - DEBUG - Iter 495/500 - Loss: -1.038\n",
      "2024-09-02 00:29:52,335 - INFO - NLL: -0.7597; RMSE: 0.2251; NMSE: 1.2332\n",
      "2024-09-02 00:29:52,335 - INFO - FOLD 4\n",
      "2024-09-02 00:29:52,338 - INFO - training starts for model <class '__main__.MultitaskGP'> with kernel <class 'gpytorch.kernels.rbf_kernel.RBFKernel'>; lr: 0.18; step_size:55; gamma:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "X_train_tensor shape: torch.Size([56, 1085])\n",
      "y_train_tensor shape: torch.Size([56, 10])\n",
      "X_train_tensor shape: torch.Size([6, 1085])\n",
      "y_train_tensor shape: torch.Size([6, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 00:29:52,926 - DEBUG - Iter 55/500 - Loss: -0.633\n",
      "2024-09-02 00:29:53,522 - DEBUG - Iter 110/500 - Loss: -0.425\n",
      "2024-09-02 00:29:54,096 - DEBUG - Iter 165/500 - Loss: -0.843\n",
      "2024-09-02 00:29:54,694 - DEBUG - Iter 220/500 - Loss: -1.070\n",
      "2024-09-02 00:29:55,300 - DEBUG - Iter 275/500 - Loss: -0.908\n",
      "2024-09-02 00:29:55,868 - DEBUG - Iter 330/500 - Loss: -0.822\n",
      "2024-09-02 00:29:56,446 - DEBUG - Iter 385/500 - Loss: -1.019\n",
      "2024-09-02 00:29:57,034 - DEBUG - Iter 440/500 - Loss: -0.997\n",
      "2024-09-02 00:29:57,613 - DEBUG - Iter 495/500 - Loss: -1.182\n",
      "2024-09-02 00:29:57,676 - INFO - NLL: -1.1945; RMSE: 0.2076; NMSE: 0.9632\n",
      "2024-09-02 00:29:57,677 - INFO - FOLD 5\n",
      "2024-09-02 00:29:57,679 - INFO - training starts for model <class '__main__.MultitaskGP'> with kernel <class 'gpytorch.kernels.rbf_kernel.RBFKernel'>; lr: 0.18; step_size:55; gamma:0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "X_train_tensor shape: torch.Size([56, 1085])\n",
      "y_train_tensor shape: torch.Size([56, 10])\n",
      "X_train_tensor shape: torch.Size([6, 1085])\n",
      "y_train_tensor shape: torch.Size([6, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 00:29:58,263 - DEBUG - Iter 55/500 - Loss: -0.425\n",
      "2024-09-02 00:29:58,851 - DEBUG - Iter 110/500 - Loss: -0.993\n",
      "2024-09-02 00:29:59,456 - DEBUG - Iter 165/500 - Loss: -0.823\n",
      "2024-09-02 00:30:00,082 - DEBUG - Iter 220/500 - Loss: -0.965\n",
      "2024-09-02 00:30:00,760 - DEBUG - Iter 275/500 - Loss: -0.839\n",
      "2024-09-02 00:30:01,436 - DEBUG - Iter 330/500 - Loss: -0.853\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m X_train_tensor,y_train_tensor\u001b[38;5;241m=\u001b[39mdataloader2tensor(train_loader)\n\u001b[0;32m     28\u001b[0m X_test_tensor,y_test_tensor\u001b[38;5;241m=\u001b[39mdataloader2tensor(val_loader)\n\u001b[1;32m---> 29\u001b[0m run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m0\u001b[39m],kernel\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m1\u001b[39m],config\u001b[38;5;241m=\u001b[39mGlobal)\n",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m, in \u001b[0;36mrun_test\u001b[1;34m(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, model, kernel, config)\u001b[0m\n\u001b[0;32m     28\u001b[0m this_loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     29\u001b[0m avg_loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mthis_loss\n\u001b[1;32m---> 30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 更新参数\u001b[39;00m\n\u001b[0;32m     31\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 更新学习率\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m%\u001b[39mSTEP_SIZE\u001b[38;5;241m==\u001b[39mSTEP_SIZE\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     adam(\n\u001b[0;32m    169\u001b[0m         params_with_grad,\n\u001b[0;32m    170\u001b[0m         grads,\n\u001b[0;32m    171\u001b[0m         exp_avgs,\n\u001b[0;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    174\u001b[0m         state_steps,\n\u001b[0;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    189\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m func(params,\n\u001b[0;32m    319\u001b[0m      grads,\n\u001b[0;32m    320\u001b[0m      exp_avgs,\n\u001b[0;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    323\u001b[0m      state_steps,\n\u001b[0;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    443\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = torch.cat((X_train_tensor, X_test_tensor), dim=0)\n",
    "y = torch.cat((y_train_tensor, y_test_tensor), dim=0)\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# 定义 KFold，n_splits=10 表示 10-fold 交叉验证\n",
    "k_folds = 10\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# 用于存储每个 fold 的结果\n",
    "fold_results = {}\n",
    "\n",
    "for each_param in params:\n",
    "        Global.lr=each_param[2]\n",
    "        Global.STEP_SIZE=each_param[3]\n",
    "        Global.gamma=each_param[4]\n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "            logger.info(f'FOLD {fold+1}')\n",
    "            print('--------------------------------')\n",
    "            \n",
    "            # 创建训练和验证数据集\n",
    "            train_subset = Subset(dataset, train_idx)\n",
    "            val_subset = Subset(dataset, val_idx)\n",
    "            \n",
    "            # 创建数据加载器\n",
    "            train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "            X_train_tensor,y_train_tensor=dataloader2tensor(train_loader)\n",
    "            X_test_tensor,y_test_tensor=dataloader2tensor(val_loader)\n",
    "            run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model=each_param[0],kernel=each_param[1],config=Global)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# param_grid = {\n",
    "#     'models':[Linear_Model_Of_Corregionalization,MultitaskGP],\n",
    "#     'kernels': [Quadriple_Kernel_Exponential_Squared],\n",
    "#     'lrs': torch.log(torch.logspace(0.003,0.3,5)).tolist(),\n",
    "#     'gammas':torch.linspace(0.2,0.8,5).tolist(),\n",
    "#     'STEP_SIZEs':torch.linspace(10,100,5).tolist()\n",
    "# }\n",
    "\n",
    "# param_grid = {\n",
    "#     'models':[Linear_Model_Of_Corregionalization],\n",
    "#     'kernels': [RBFKernel],\n",
    "#     'lrs': [0.69 ],\n",
    "#     'gammas':[0.5],\n",
    "#     'STEP_SIZEs':[33]\n",
    "# }\n",
    "\n",
    "\n",
    "# param_combinations = list(product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 11:26:49,720 - INFO - training starts for model <class '__main__.Linear_Model_Of_Corregionalization'> with kernel <class 'gpytorch.kernels.rbf_kernel.RBFKernel'>; lr: 0.69; step_size:33; gamma:0.5\n",
      "2024-09-01 11:26:49,725 - DEBUG - Completed\n",
      "2024-09-01 11:26:52,220 - DEBUG - Iter 33/500 - Loss: 65.768\n",
      "2024-09-01 11:26:54,394 - DEBUG - Iter 66/500 - Loss: 55.813\n",
      "2024-09-01 11:26:56,605 - DEBUG - Iter 99/500 - Loss: 52.295\n",
      "2024-09-01 11:26:58,892 - DEBUG - Iter 132/500 - Loss: 50.517\n",
      "2024-09-01 11:27:01,124 - DEBUG - Iter 165/500 - Loss: 49.588\n",
      "2024-09-01 11:27:01,124 - INFO - Early cut off at epoch 164 with loss of 49.58796691894531\n",
      "2024-09-01 11:27:01,194 - INFO - NLL: 75.4033; RMSE: 1051.4624; NMSE: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# for each_param in param_combinations:\n",
    "#     Global.lr=each_param[2]\n",
    "#     Global.gamma=each_param[3]\n",
    "#     Global.STEP_SIZE=each_param[4]\n",
    "#     run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model=each_param[0],kernel=each_param[1],config=Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TOY_DATASET == True:\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(X_test_tensor))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize plot\n",
    "        f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        mean=observed_pred.mean.numpy()\n",
    "        for i in range(3):\n",
    "            ax[i].plot(X_test_tensor[i*50:(i+1)*50,1:].squeeze().numpy(),mean[i*50:(i+1)*50], 'k*')\n",
    "            ax[i].fill_between(X_test_tensor[i*50:(i+1)*50,1:].flatten().numpy(), lower[i*50:(i+1)*50].squeeze().numpy(), upper[i*50:(i+1)*50].squeeze().numpy(), alpha=0.5)\n",
    "            ax[i].set_ylim([-3, 3])\n",
    "            ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
