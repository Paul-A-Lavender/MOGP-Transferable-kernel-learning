{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gpytorch.kernels import Kernel\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from gpytorch.means import Mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from gpytorch.kernels import Kernel, RBFKernel,ScaleKernel\n",
    "from gpytorch.constraints import Interval\n",
    "from gpytorch.settings import cholesky_jitter\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution, VariationalStrategy\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "from itertools import product\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StreamHandler added\n",
      "GPU is not available!\n"
     ]
    }
   ],
   "source": [
    "# 创建一个logger\n",
    "logger = logging.getLogger('my_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建文件处理器，写入日志文件\n",
    "fh = logging.FileHandler('app.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "\n",
    "# 设置日志格式\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "\n",
    "# 将处理器添加到logger中\n",
    "# 创建控制台处理器，输出到控制台\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "if not any(isinstance(handler, logging.StreamHandler) for handler in logger.handlers):\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "    print(\"StreamHandler added\")\n",
    "else:\n",
    "    print(\"StreamHandler already exists\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available!\")\n",
    "USE_TOY_DATASET=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(X_path,y_path,X_domain_path=None,do_standardisation=False,test_size=0.1,random_state=42):\n",
    "    X_df=None\n",
    "    X_domain_info=None\n",
    "    y_df=None\n",
    "    with open(X_path, 'rb') as f:\n",
    "        X_df = pickle.load(f)\n",
    "    with open(y_path, 'rb') as f:\n",
    "        y_df = pickle.load(f)\n",
    "    print(X_df.shape)\n",
    "    print(y_df.shape)\n",
    "    if X_domain_path!=None:\n",
    "        with open(X_domain_path, 'rb') as f:\n",
    "            X_domain_info = pickle.load(f)\n",
    "        print(X_domain_info.shape)\n",
    "        X_train, X_test, y_train, y_test,X_D_train,X_D_test = train_test_split(X_df, y_df,X_domain_info, test_size=test_size, random_state=random_state)\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test= train_test_split(X_df, y_df, test_size=test_size, random_state=random_state)\n",
    "    X_train = np.array(X_train, dtype=np.float32)\n",
    "    y_train = np.array(y_train, dtype=np.float32)\n",
    "\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train = np.array(X_D_train, dtype=np.float32)\n",
    "        X_D_test = np.array(X_D_test, dtype=np.float32)\n",
    "\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    y_test = np.array(y_test, dtype=np.float32)\n",
    "    if do_standardisation:\n",
    "        print(\"Performing standardisation\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train)\n",
    "    y_train_tensor = torch.tensor(y_train)\n",
    "    if X_domain_path!=None:\n",
    "        X_D_train_tensor = torch.tensor(X_D_train)\n",
    "        X_D_test_tensor = torch.tensor(X_D_test)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test)\n",
    "    y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "    if X_domain_path!=None: \n",
    "        return (X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)\n",
    "    else:\n",
    "        return (X_train_tensor,y_train_tensor),(X_test_tensor,y_test_tensor)\n",
    "\n",
    "def sigmoid_4_param(x, x0, L, k, d):\n",
    "    \"\"\" Comparing with Dennis Wang's sigmoid:\n",
    "    x0 -  p - position, correlation with IC50 or EC50\n",
    "        bounds [0, 1]\n",
    "    L = 1 in Dennis Wang's sigmoid, protect from devision by zero if x is too small \n",
    "        L<1 inverted sigmoid, l=100 - lower upper and lower boundso sigmpoid on y axis (y= [0.1, 0.11])\n",
    "        bounds [0.8, 10]\n",
    "    k = -1/s (s -shape parameter)  default = -10 k=0 straight line, k<0 sigmoid around k=-10\n",
    "        bounds [1, -100]\n",
    "    d - determines the vertical position of the sigmoid - shift on y axis - better fitting then Dennis Wang's sigmoid\n",
    "         bounds [0, 0.9]\n",
    "    parameters_bound ((0, 0.8, -100, 0), (1, 10, 1, 0.9))\n",
    "    \"\"\"\n",
    "    return ( 1/ (L + np.exp(-k*(x-x0))) + d)\n",
    "\n",
    "def overwrite_to_test():\n",
    "    VAR=0.09\n",
    "    AMP=1.0\n",
    "    train_size=100\n",
    "\n",
    "    X_sin=torch.linspace(0, 1, train_size)\n",
    "    X_cos=torch.linspace(0, 1, train_size)\n",
    "    X_sig=torch.linspace(0, 1, train_size)\n",
    "    y_sin = AMP*torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos = AMP*torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig = AMP*(torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    # Training data is 100 points in [0,1] inclusive regularly spaced\n",
    "    X_train_tensor = torch.linspace(0, 1, train_size)\n",
    "    # True function is sin(2*pi*x) with Gaussian noise\n",
    "\n",
    "    num_conc=1\n",
    "    num_feat=1\n",
    "    nums_domain=torch.Tensor([3])\n",
    "    num_data=train_size\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "    y_sin = y_sin.unsqueeze(1)\n",
    "    y_cos = y_cos.unsqueeze(1)\n",
    "    y_sig = y_sig.unsqueeze(1)\n",
    "\n",
    "    X_sin_domain=torch.zeros(train_size,1)\n",
    "    X_cos_domain=torch.ones(train_size,1)\n",
    "    X_sig_domain=torch.ones(train_size,1)*2\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "    X_train_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_train_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    for i in range(3):\n",
    "        ax[i].plot(X_train_tensor[i*train_size:(i+1)*train_size,1:].squeeze().numpy(),y_train_tensor[i*train_size:(i+1)*train_size], 'k*')\n",
    "        axis=X_train_tensor[i*train_size:(i+1)*train_size,1:].flatten().numpy()\n",
    "        ax[i].set_ylim([-3, 3])\n",
    "        ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "\n",
    "    # indices = torch.randperm(X_train_tensor.size(0))\n",
    "    # X_train_tensor = X_train_tensor[indices]\n",
    "    # y_train_tensor = y_train_tensor[indices]\n",
    "\n",
    "    test_size=50\n",
    "    X_sin=torch.linspace(0, 1, test_size)\n",
    "    X_cos=torch.linspace(0, 1, test_size)\n",
    "    X_sig=torch.linspace(0, 1, test_size)\n",
    "\n",
    "    X_sin_domain=torch.zeros(test_size,1)\n",
    "    X_cos_domain=torch.ones(test_size,1)\n",
    "    X_sig_domain=torch.ones(test_size,1)*2\n",
    "\n",
    "    X_sin = X_sin.unsqueeze(1)\n",
    "    X_cos = X_cos.unsqueeze(1)\n",
    "    X_sig = X_sig.unsqueeze(1)\n",
    "\n",
    "    y_sin =AMP* torch.sin(X_sin * (2 * math.pi)) + torch.randn(X_sin.size()) * math.sqrt(VAR)\n",
    "    y_cos =AMP* torch.cos(X_cos * (2 * math.pi)) + torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "    y_sig =AMP* (torch.sigmoid(15*(X_sig-0.5))-0.5)*2+torch.randn(X_cos.size()) * math.sqrt(VAR)\n",
    "\n",
    "    X_sin_cat = torch.cat((X_sin_domain, X_sin), dim=1)\n",
    "    X_cos_cat = torch.cat((X_cos_domain, X_cos), dim=1)\n",
    "    X_sig_cat = torch.cat((X_sig_domain, X_sig), dim=1)\n",
    "\n",
    "    # X_test_tensor =X_sin_cat\n",
    "    # y_test_tensor =y_sin\n",
    "    \n",
    "    X_test_tensor = torch.cat((X_sin_cat, X_cos_cat,X_sig_cat), dim=0)\n",
    "    y_test_tensor = torch.cat((y_sin, y_cos,y_sig), dim=0)\n",
    "\n",
    "    # X_test_tensor = torch.cat((X_sin_cat, X_cos_cat), dim=0)\n",
    "    # y_test_tensor = torch.cat((y_sin, y_cos), dim=0)  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    NUMS_DOMAIN=None\n",
    "    NUMS_DOMAIN_FEATURE=None\n",
    "    NUMS_DOMAIN_AS_INT=None\n",
    "    NUM_CONC=None\n",
    "    STEP_SIZE=None\n",
    "    lr=None\n",
    "    gamma=None\n",
    "    def __init__(self, NUMS_DOMAIN=None,\n",
    "                 NUMS_DOMAIN_FEATURE=None,\n",
    "                 NUMS_DOMAIN_AS_INT=None,\n",
    "                 NUM_CONC=None,\n",
    "                 STEP_SIZE=None,\n",
    "                 lr=None,\n",
    "                 gamma=None,\n",
    "                 NUM_FEAT=None,):\n",
    "        self.NUMS_DOMAIN=NUMS_DOMAIN\n",
    "        self.NUMS_DOMAIN_FEATURE=NUMS_DOMAIN_FEATURE\n",
    "        self.NUMS_DOMAIN_AS_INT=NUMS_DOMAIN_AS_INT\n",
    "        self.NUM_CONC=NUM_CONC\n",
    "        self.STEP_SIZE=STEP_SIZE\n",
    "        self.lr=lr\n",
    "        self.gamma=gamma\n",
    "        self.NUM_FEAT=NUM_FEAT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model,kernel,config):\n",
    "    logger.info(f'training starts for model {str(model)} with kernel {str(kernel)}; lr: {config.lr}; step_size:{config.STEP_SIZE}; gamma:{config.gamma}')\n",
    "    likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=config.NUM_CONC)\n",
    "\n",
    "    m = model(X_train_tensor, y_train_tensor, likelihood,kernel,config)\n",
    "\n",
    "    training_iterations = 500\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    m.train()\n",
    "    likelihood.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(m.parameters(), lr=config.lr)\n",
    "    STEP_SIZE=config.STEP_SIZE\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=config.gamma)\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, m)\n",
    "    last_loss=1\n",
    "    avg_loss=0\n",
    "    this_loss=0\n",
    "    for i in range(training_iterations):\n",
    "        # try:\n",
    "            optimizer.zero_grad()\n",
    "            output = m(X_train_tensor)\n",
    "            loss = -mll(output, y_train_tensor)\n",
    "            loss.backward()\n",
    "                \n",
    "            this_loss=loss.item()\n",
    "            avg_loss+=this_loss\n",
    "            optimizer.step()  # 更新参数\n",
    "            scheduler.step()  # 更新学习率\n",
    "            \n",
    "            if i%STEP_SIZE==STEP_SIZE-1:\n",
    "                logger.debug('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "                avg_loss=avg_loss/STEP_SIZE\n",
    "                if abs((this_loss-avg_loss)/avg_loss)<0.001:\n",
    "                    logger.info(f'Early cut off at epoch {i} with loss of {this_loss }')\n",
    "                    break\n",
    "                        \n",
    "                avg_loss=0\n",
    "                    \n",
    "        # except Exception as e:\n",
    "        #     logger.error(f\"发生了一个异常: {e}\")\n",
    "        #     continue\n",
    "        \n",
    "    m.eval()\n",
    "    likelihood.eval()\n",
    "    # Make predictions\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        distribution = likelihood(m(X_test_tensor))\n",
    "        mean = distribution.mean\n",
    "        lower, upper = distribution.confidence_region()\n",
    "    nll = -torch.distributions.Normal(mean, distribution.variance.sqrt()).log_prob(y_test_tensor).mean().item()\n",
    "\n",
    "    rmse=torch.sqrt(torch.tensor(mean_squared_error(y_test_tensor.numpy(), mean.numpy()))).item() \n",
    "    nmse = rmse / torch.var(y_test_tensor).item()\n",
    "\n",
    "    logger.info(f'NLL: {nll:.4f}; RMSE: {rmse:.4f}; NMSE: {nmse:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Exponential Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel_Exponential_Squared(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.height_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN)))\n",
    "        self.length_scales=torch.nn.Parameter(torch.ones((config.NUMS_DOMAIN))) \n",
    "        self.register_parameter(name='height_scales', parameter=self.height_scales)\n",
    "        self.register_parameter(name='length_scales', parameter=self.length_scales)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        # x1_domain   =   x1[:, :num_domain_feat]\n",
    "        # x1_genetics =   x1[:, num_domain_feat:]\n",
    "        # x2_domain   =   x2[:, :num_domain_feat]\n",
    "        # x2_genetics =   x2[:, num_domain_feat:]\n",
    "\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetics =   x1\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_genetics =   x2\n",
    "\n",
    "        dist_mat=self.covar_dist(x1_genetics, x2_genetics, square_dist=True, diag=diag, **params)\n",
    "        height_scales_parsed_1 = self.height_scales[x1_domain.long()].flatten()\n",
    "        height_scales_parsed_2 = self.height_scales[x2_domain.long()].flatten()\n",
    "        length_scales_parsed_1 = self.length_scales[x1_domain.long()].flatten()\n",
    "        length_scales_parsed_2 = self.length_scales[x2_domain.long()].flatten()\n",
    "\n",
    "        part_L1L2T=torch.sqrt(torch.outer(length_scales_parsed_1*length_scales_parsed_1,length_scales_parsed_2.T*length_scales_parsed_2.T))\n",
    "        part_L1L1T=length_scales_parsed_1*length_scales_parsed_1\n",
    "        part_L2L2T=length_scales_parsed_2*length_scales_parsed_2\n",
    "        part_L1sqrL2sqr=torch.outer(part_L1L1T,torch.ones_like(part_L2L2T).T)+torch.outer(torch.ones_like(part_L1L1T),part_L2L2T)\n",
    "\n",
    "        part_1=torch.outer(height_scales_parsed_1,height_scales_parsed_2.T)\n",
    "        part_2=torch.sqrt(2*part_L1L2T/part_L1sqrL2sqr)\n",
    "        part_3=torch.exp(-dist_mat/part_L1sqrL2sqr)\n",
    "\n",
    "        result=part_1*part_2*part_3\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_MS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class K_MS(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.domain_coefficient=torch.nn.Parameter(torch.rand((config.NUMS_DOMAIN))*2-1)\n",
    "        self.K_ES=Kernel_Exponential_Squared(config)\n",
    "        self.register_parameter(name='domain_coefficient', parameter=self.domain_coefficient)\n",
    "        self.config=config\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "\n",
    "        domain_coefficient_parsed_1=self.domain_coefficient[x1_domain.long()].flatten()\n",
    "        domain_coefficient_parsed_2=self.domain_coefficient[x2_domain.long()].flatten()\n",
    "        domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
    "\n",
    "        domain_mat=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat == 1.0)\n",
    "\n",
    "        # 等于1的位置置0\n",
    "        domain_mat[mask] = 0.0\n",
    "\n",
    "        # 不等于1的位置置1\n",
    "        domain_mat[~mask] = 1.0\n",
    "        # print(domain_scaler)\n",
    "\n",
    "        \n",
    "        base_cov=self.K_ES(x1,x2,diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
    "\n",
    "        final_scaler=(domain_scaler-1.0)*domain_mat+1.0\n",
    "        return base_cov*final_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K_Alpha_Beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_Alpha_Beta(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernels=[]\n",
    "        for _ in range(config.NUMS_DOMAIN):\n",
    "            self.kernels.append(RBFKernel())\n",
    "        self.config=config\n",
    "        self.alpha= torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "        self.beta=torch.nn.Parameter(torch.ones(config.NUMS_DOMAIN))\n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):#\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_num_data=x1_domain.shape[0]\n",
    "        x2_num_data=x2_domain.shape[0]\n",
    "        cov_mats_alpha=torch.zeros([x1_num_data,x2_num_data])\n",
    "        cov_mats_beta=torch.zeros([x1_num_data,x2_num_data])\n",
    "\n",
    "        alpha_reparameterized=torch.sqrt(self.alpha*self.alpha)\n",
    "        beta_reparameterized=torch.sqrt(self.beta*self.beta)\n",
    "        for i in range(self.config.NUMS_DOMAIN):\n",
    "            cov_mats_alpha=cov_mats_alpha.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*alpha_reparameterized[i])\n",
    "            cov_mats_beta=cov_mats_beta.add(self.kernels[i](x1, x2, diag=diag, last_dim_is_batch=last_dim_is_batch, **params)*beta_reparameterized[i])\n",
    "\n",
    "        domain_mat_alpha=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        domain_mat_beta=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat_alpha == 1.0)\n",
    "        domain_mat_alpha[mask] = 0.0\n",
    "        domain_mat_alpha[~mask] = 1.0\n",
    "        domain_mat_beta[mask] = 1.0\n",
    "        domain_mat_beta[~mask] = 0.0\n",
    "\n",
    "        \n",
    "        result=cov_mats_alpha*domain_mat_alpha+cov_mats_beta*domain_mat_beta\n",
    "        if diag:\n",
    "            return result.diag()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferable quadriple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "class K_MS_with_Feat_Scaling(Kernel):\n",
    "    def __init__(self,config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.domain_coefficient=torch.nn.Parameter(torch.rand(config.NUMS_DOMAIN)*2-1)\n",
    "        self.K_ES=Kernel_Exponential_Squared(config)\n",
    "        self.register_parameter(name='domain_coefficient', parameter=self.domain_coefficient)\n",
    "        \n",
    "        \n",
    "        feature_relateness_init=torch.ones(config.NUM_FEAT)\n",
    "        self.feature_relateness=torch.nn.Parameter(feature_relateness_init)\n",
    "        self.register_parameter(name='feature_relateness', parameter=self.feature_relateness)\n",
    "        self.config=config\n",
    "        \n",
    "    \n",
    "    def forward(self, x1, x2, diag=False, last_dim_is_batch=False, **params):\n",
    "        x1_domain   =   x1[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x2_domain   =   x2[:, :self.config.NUMS_DOMAIN_FEATURE]\n",
    "        x1_genetic   =   x1[:, self.config.NUMS_DOMAIN_FEATURE:]\n",
    "        x2_genetic   =   x2[:, self.config.NUMS_DOMAIN_FEATURE:]        \n",
    "        \n",
    "        feature_scaler_rep=(torch.tanh(self.feature_relateness)+1.0)\n",
    "        x1_scaled=x1_genetic*torch.outer(torch.ones(x1.shape[0]),feature_scaler_rep)\n",
    "        x2_scaled=x2_genetic*torch.outer(torch.ones(x2.shape[0]),feature_scaler_rep)\n",
    "        domain_coefficient_parsed_1=self.domain_coefficient[x1_domain.long()].flatten()\n",
    "        domain_coefficient_parsed_2=self.domain_coefficient[x2_domain.long()].flatten()\n",
    "        domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n",
    "        domain_mat=torch.outer(x1_domain.flatten()+1,1/(x2_domain.flatten().T+1))\n",
    "        mask = (domain_mat == 1.0)\n",
    "\n",
    "        # 等于1的位置置0\n",
    "        domain_mat[mask] = 0.0\n",
    "\n",
    "        # 不等于1的位置置1\n",
    "        domain_mat[~mask] = 1.0\n",
    "        # print(domain_scaler)\n",
    "        x1_cat=torch.cat((x1_domain, x1_scaled), dim=1)\n",
    "        x2_cat=torch.cat((x2_domain, x2_scaled), dim=1)\n",
    "        base_cov=self.K_ES(x1_scaled,x2_scaled,diag=diag, last_dim_is_batch=last_dim_is_batch, **params)\n",
    "\n",
    "        final_scaler=(domain_scaler-1.0)*domain_mat+1.0\n",
    "        return base_cov*final_scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model of corregionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Model_Of_Corregionalization(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(Linear_Model_Of_Corregionalization, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        if kernel==RBFKernel:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel())\n",
    "        else:\n",
    "            for _ in range(config.NUM_CONC):\n",
    "                kernels.append(kernel(config))\n",
    "        self.covar_module =gpytorch.kernels.LCMKernel(\n",
    "            kernels, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "\n",
    "        logger.debug(\"Completed\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskGP(gpytorch.models.ExactGP):\n",
    "    #TODO add input for domain information\n",
    "    def __init__(self, train_x, train_y,likelihood,kernel,config):\n",
    "        super(MultitaskGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.MultitaskMean(\n",
    "            gpytorch.means.ConstantMean(), num_tasks=config.NUM_CONC\n",
    "        )\n",
    "        kernels=[]\n",
    "        \n",
    "        if kernel==RBFKernel:\n",
    "            kern=kernel()\n",
    "        else:\n",
    "            kern=kernel(config)\n",
    "                \n",
    "        self.covar_module =gpytorch.kernels.MultitaskKernel(\n",
    "            kern, num_tasks=config.NUM_CONC, rank=config.NUM_CONC\n",
    "        )\n",
    "        #     self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "        #         K_MS(),  # 2 latent GP's\n",
    "\n",
    "        #     num_tasks=num_conc, rank=num_conc-1\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153, 1084)\n",
      "(153, 10)\n",
      "(153, 1)\n",
      "torch.Size([137, 1084])\n",
      "torch.Size([137, 1])\n",
      "torch.Size([137, 10])\n",
      "NUMS_DOMAIN: 8.0\n",
      "torch.Size([137, 1085])\n",
      "torch.Size([16, 1085])\n",
      "MUT features:310.0torch.Size([1084, 1084])\n",
      "Met features:338.0torch.Size([1084, 1084])\n",
      "CNA features:425.0torch.Size([1084, 1084])\n",
      "DC features:0.0torch.Size([1084, 1084])\n"
     ]
    }
   ],
   "source": [
    "# X_path=\"data/X_df_toy.pkl\"\n",
    "# y_path=\"data/y_df_toy.pkl\"\n",
    "\n",
    "# X_path=\"data/X_df_Shikonin.pkl\"\n",
    "# y_path=\"data/y_df_Shikonin.pkl\"\n",
    "# X_domain_path=\"data/X_domain_info_Shikonin.pkl\"\n",
    "\n",
    "Global=config()\n",
    "\n",
    "# X_path=\"data/X_df_2_shots.pkl\"\n",
    "# y_path=\"data/y_df_2_shots.pkl\"\n",
    "# X_domain_path=\"data/X_domain_info_2_shots.pkl\"\n",
    "\n",
    "# X_path=\"data/2_shots_4_params_sigmoid/X_df.pkl\"\n",
    "# y_path=\"data/2_shots_4_params_sigmoid/y_df.pkl\"\n",
    "# X_domain_path=\"data/2_shots_4_params_sigmoid/X_domain_info.pkl\"\n",
    "\n",
    "X_path=\"data/8_shots/X_df.pkl\"\n",
    "y_path=\"data/8_shots/y_df.pkl\"\n",
    "X_domain_path=\"data/8_shots/X_domain_info.pkl\"\n",
    "(X_train_tensor,X_D_train_tensor,y_train_tensor),(X_test_tensor,X_D_test_tensor,y_test_tensor)=load_dataset(X_path,y_path,X_domain_path=X_domain_path)\n",
    "\n",
    "print(X_train_tensor.shape)\n",
    "print(X_D_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n",
    "Global.NUM_CONC=y_train_tensor.shape[1]\n",
    "\n",
    "Global.NUM_FEAT=X_train_tensor.shape[1]\n",
    "Global.NUM_DOMAIN_FEAT=X_D_train_tensor.shape[1]\n",
    "NUMS_DOMAIN, max_indices_row = torch.max(X_D_train_tensor, dim=0)\n",
    "NUMS_DOMAIN.add_(1)\n",
    "\n",
    "\n",
    "print(f'NUMS_DOMAIN: {NUMS_DOMAIN.item()}')\n",
    "Global.NUMS_DOMAIN=NUMS_DOMAIN.long()\n",
    "X_train_tensor = torch.cat((X_D_train_tensor, X_train_tensor), dim=1)\n",
    "X_test_tensor = torch.cat((X_D_test_tensor, X_test_tensor), dim=1)\n",
    "print(X_train_tensor.shape)\n",
    "print(X_test_tensor.shape)\n",
    "\n",
    "with open(X_path, 'rb') as f:\n",
    "    X_df = pickle.load(f)\n",
    "mask_bool_met = X_df.columns.str.contains(\"HypMET\")\n",
    "mask_bool_mut = X_df.columns.str.contains(\"mut\")\n",
    "mask_bool_cna = X_df.columns.str.contains(\"cna\")\n",
    "mask_bool_dc  = X_df.columns.str.contains(\"dc\")\n",
    "\n",
    "mask_float_met = np.array(mask_bool_met, dtype=np.float32) \n",
    "mask_float_mut = np.array(mask_bool_mut, dtype=np.float32)\n",
    "mask_float_cna = np.array(mask_bool_cna, dtype=np.float32)\n",
    "mask_float_dc  = np.array(mask_bool_dc, dtype=np.float32)\n",
    "\n",
    "diag_matrix_met = torch.tensor(np.diag(mask_float_met))\n",
    "diag_matrix_mut = torch.tensor(np.diag(mask_float_mut))\n",
    "diag_matrix_cna = torch.tensor(np.diag(mask_float_cna))\n",
    "diag_matrix_dc  = torch.tensor(np.diag(mask_float_dc))\n",
    "print(\"MUT features:\"+str(np.sum(mask_float_mut))+str(diag_matrix_met.shape))\n",
    "print(\"Met features:\"+str(np.sum(mask_float_met))+str(diag_matrix_met.shape))\n",
    "print(\"CNA features:\"+str(np.sum(mask_float_cna))+str(diag_matrix_met.shape))\n",
    "print(\"DC features:\"+str(np.sum(mask_float_dc))+str(diag_matrix_met.shape))\n",
    "filters=(diag_matrix_mut,diag_matrix_met,diag_matrix_cna,diag_matrix_dc)\n",
    "Global.filters=filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Global.lr=0.1\n",
    "Global.gamma=0.5\n",
    "Global.STEP_SIZE=50\n",
    "Global.NUMS_DOMAIN_FEATURE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# param_grid = {\n",
    "#     'models':[Linear_Model_Of_Corregionalization,MultitaskGP],\n",
    "#     'kernels': [Quadriple_Kernel_Exponential_Squared],\n",
    "#     'lrs': torch.log(torch.logspace(0.003,0.3,5)).tolist(),\n",
    "#     'gammas':torch.linspace(0.2,0.8,5).tolist(),\n",
    "#     'STEP_SIZEs':torch.linspace(10,100,5).tolist()\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'models':[Linear_Model_Of_Corregionalization],\n",
    "    'kernels': [K_MS_with_Feat_Scaling],\n",
    "    'lrs': [0.3],\n",
    "    'gammas':[0.5],\n",
    "    'STEP_SIZEs':[50]\n",
    "}\n",
    "\n",
    "\n",
    "param_combinations = list(product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-22 16:16:55,083 - INFO - training starts for model <class '__main__.Linear_Model_Of_Corregionalization'> with kernel <class '__main__.K_MS_with_Feat_Scaling'>; lr: 0.3; step_size:50; gamma:0.5\n",
      "2024-08-22 16:16:55,106 - DEBUG - Completed\n",
      "C:\\Users\\25858\\AppData\\Local\\Temp\\ipykernel_135744\\1318098534.py:27: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\b\\abs_8f7uhuge1i\\croot\\pytorch-select_1717607507421\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3679.)\n",
      "  domain_scaler=torch.outer(torch.tanh(domain_coefficient_parsed_1),torch.tanh(domain_coefficient_parsed_2.T))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 28 is out of bounds for dimension 0 with size 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m Global\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m      4\u001b[0m Global\u001b[38;5;241m.\u001b[39mSTEP_SIZE\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m0\u001b[39m],kernel\u001b[38;5;241m=\u001b[39meach_param[\u001b[38;5;241m1\u001b[39m],config\u001b[38;5;241m=\u001b[39mGlobal)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mrun_test\u001b[1;34m(X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, model, kernel, config)\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     24\u001b[0m output \u001b[38;5;241m=\u001b[39m m(X_train_tensor)\n\u001b[1;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_train_tensor)\n\u001b[0;32m     26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m this_loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\mlls\\exact_marginal_log_likelihood.py:66\u001b[0m, in \u001b[0;36mExactMarginalLogLikelihood.forward\u001b[1;34m(self, function_dist, target, *params, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactMarginalLogLikelihood can only operate on Gaussian random variables\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Determine output likelihood\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlikelihood(function_dist, \u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Remove NaN values if enabled\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mobservation_nan_policy\u001b[38;5;241m.\u001b[39mvalue() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\likelihoods\\likelihood.py:76\u001b[0m, in \u001b[0;36m_Likelihood.__call__\u001b[1;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Marginal\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, MultivariateNormal):\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarginal(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Error\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLikelihoods expects a MultivariateNormal input to make marginal predictions, or a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor for conditional predictions. Got a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\likelihoods\\multitask_gaussian_likelihood.py:299\u001b[0m, in \u001b[0;36mMultitaskGaussianLikelihood.marginal\u001b[1;34m(self, function_dist, *args, **kwargs)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarginal\u001b[39m(\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m, function_dist: MultitaskMultivariateNormal, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    295\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m MultitaskMultivariateNormal:\n\u001b[0;32m    296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    :return: Analytic marginal :math:`p(\\mathbf y)`.\u001b[39;00m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mmarginal(function_dist, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\likelihoods\\multitask_gaussian_likelihood.py:107\u001b[0m, in \u001b[0;36m_MultitaskGaussianLikelihoodBase.marginal\u001b[1;34m(self, function_dist, *params, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# ensure that sumKroneckerLT is actually called\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(covar, LazyEvaluatedKernelTensor):\n\u001b[1;32m--> 107\u001b[0m     covar \u001b[38;5;241m=\u001b[39m covar\u001b[38;5;241m.\u001b[39mevaluate_kernel()\n\u001b[0;32m    109\u001b[0m covar_kron_lt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shaped_noise_covar(\n\u001b[0;32m    110\u001b[0m     mean\u001b[38;5;241m.\u001b[39mshape, add_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_global_noise, interleaved\u001b[38;5;241m=\u001b[39mfunction_dist\u001b[38;5;241m.\u001b[39m_interleaved\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m covar \u001b[38;5;241m=\u001b[39m covar \u001b[38;5;241m+\u001b[39m covar_kron_lt\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\utils\\memoize.py:59\u001b[0m, in \u001b[0;36m_cached.<locals>.g\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m kwargs_pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mdumps(kwargs)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_in_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _add_to_cache(\u001b[38;5;28mself\u001b[39m, cache_name, method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_from_cache(\u001b[38;5;28mself\u001b[39m, cache_name, \u001b[38;5;241m*\u001b[39margs, kwargs_pkl\u001b[38;5;241m=\u001b[39mkwargs_pkl)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:25\u001b[0m, in \u001b[0;36mrecall_grad_state.<locals>.wrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_grad_enabled):\n\u001b[1;32m---> 25\u001b[0m         output \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\lazy\\lazy_evaluated_kernel_tensor.py:355\u001b[0m, in \u001b[0;36mLazyEvaluatedKernelTensor.evaluate_kernel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m     temp_active_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel(\n\u001b[0;32m    356\u001b[0m         x1,\n\u001b[0;32m    357\u001b[0m         x2,\n\u001b[0;32m    358\u001b[0m         diag\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    359\u001b[0m         last_dim_is_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dim_is_batch,\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams,\n\u001b[0;32m    361\u001b[0m     )\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel\u001b[38;5;241m.\u001b[39mactive_dims \u001b[38;5;241m=\u001b[39m temp_active_dims\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check the size of the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 530\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Kernel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x1_, x2_, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\lcm_kernel.py:57\u001b[0m, in \u001b[0;36mLCMKernel.forward\u001b[1;34m(self, x1, x2, **params)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m---> 57\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mforward(x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_module_list[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     59\u001b[0m         res \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward(x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\multitask_kernel.py:52\u001b[0m, in \u001b[0;36mMultitaskKernel.forward\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]):\n\u001b[0;32m     51\u001b[0m     covar_i \u001b[38;5;241m=\u001b[39m covar_i\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m*\u001b[39mx1\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m covar_x \u001b[38;5;241m=\u001b[39m to_linear_operator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_covar_module\u001b[38;5;241m.\u001b[39mforward(x1, x2, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams))\n\u001b[0;32m     53\u001b[0m res \u001b[38;5;241m=\u001b[39m KroneckerProductLinearOperator(covar_x, covar_i)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mdiagonal(dim1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, dim2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m diag \u001b[38;5;28;01melse\u001b[39;00m res\n",
      "Cell \u001b[1;32mIn[9], line 39\u001b[0m, in \u001b[0;36mK_MS_with_Feat_Scaling.forward\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m     37\u001b[0m x1_cat\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((x1_domain, x1_scaled), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m x2_cat\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((x2_domain, x2_scaled), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m base_cov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK_ES(x1_scaled,x2_scaled,diag\u001b[38;5;241m=\u001b[39mdiag, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m     41\u001b[0m final_scaler\u001b[38;5;241m=\u001b[39m(domain_scaler\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\u001b[38;5;241m*\u001b[39mdomain_mat\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m base_cov\u001b[38;5;241m*\u001b[39mfinal_scaler\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\kernels\\kernel.py:530\u001b[0m, in \u001b[0;36mKernel.__call__\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m    527\u001b[0m     res \u001b[38;5;241m=\u001b[39m LazyEvaluatedKernelTensor(x1_, x2_, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     res \u001b[38;5;241m=\u001b[39m to_linear_operator(\n\u001b[1;32m--> 530\u001b[0m         \u001b[38;5;28msuper\u001b[39m(Kernel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x1_, x2_, last_dim_is_batch\u001b[38;5;241m=\u001b[39mlast_dim_is_batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\25858\\anaconda3\\Lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mKernel_Exponential_Squared.forward\u001b[1;34m(self, x1, x2, diag, last_dim_is_batch, **params)\u001b[0m\n\u001b[0;32m     18\u001b[0m x2_genetics \u001b[38;5;241m=\u001b[39m   x2\n\u001b[0;32m     20\u001b[0m dist_mat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovar_dist(x1_genetics, x2_genetics, square_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, diag\u001b[38;5;241m=\u001b[39mdiag, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 21\u001b[0m height_scales_parsed_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight_scales[x1_domain\u001b[38;5;241m.\u001b[39mlong()]\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     22\u001b[0m height_scales_parsed_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight_scales[x2_domain\u001b[38;5;241m.\u001b[39mlong()]\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     23\u001b[0m length_scales_parsed_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_scales[x1_domain\u001b[38;5;241m.\u001b[39mlong()]\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 28 is out of bounds for dimension 0 with size 8"
     ]
    }
   ],
   "source": [
    "for each_param in param_combinations:\n",
    "    Global.lr=each_param[2]\n",
    "    Global.gamma=each_param[3]\n",
    "    Global.STEP_SIZE=each_param[4]\n",
    "    run_test(X_train_tensor,y_train_tensor,X_test_tensor,y_test_tensor,model=each_param[0],kernel=each_param[1],config=Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TOY_DATASET == True:\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Test points are regularly spaced along [0,1]\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(X_test_tensor))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Initialize plot\n",
    "        f, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "        lower, upper = observed_pred.confidence_region()\n",
    "        mean=observed_pred.mean.numpy()\n",
    "        for i in range(3):\n",
    "            ax[i].plot(X_test_tensor[i*50:(i+1)*50,1:].squeeze().numpy(),mean[i*50:(i+1)*50], 'k*')\n",
    "            ax[i].fill_between(X_test_tensor[i*50:(i+1)*50,1:].flatten().numpy(), lower[i*50:(i+1)*50].squeeze().numpy(), upper[i*50:(i+1)*50].squeeze().numpy(), alpha=0.5)\n",
    "            ax[i].set_ylim([-3, 3])\n",
    "            ax[i].legend(['Observed Data', 'Mean', 'Confidence'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
